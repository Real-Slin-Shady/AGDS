# (APPENDIX) Appendix {-}

# Solutions {#solutions}

## Getting Started

### Dimensions of a circle {-}

- Given the radius of a circle write a few lines of code that calculates its area and its circumference. Run your code with different values assigned of the radius. 
```{r}
radius <- 1
area <- pi * radius^2
circum <- 2 * pi * radius
```
- Print the solution as text.
```{r}
print(paste("Radius:", radius, "   Circumference:", circum))
```

### Sequence of numbers {-}

Generate a sequence of numbers from 0 and $\pi$ as a vector with length 5.
```{r}
seq(0, pi, length.out = 5)
```

### Gauss sum {-}

Rumors have it that young Carl Friedrich Gauss was asked in primary school to calculate the sum of all natural numbers between 1 and 100. He did it in his head in no time. We're very likely not as intelligent as young Gauss. But we have R. What's the solution?

```{r}
sum(1:100)
```

Gauss calculated the sum with a trick. The sum of 100 and 1 is 101. The sum of 99 and 2 is 101. You do this 50 times, and you get $50 \times 101$. Demonstrate Gauss' trick with vectors in R.
```{r}
vec_a <- 1:50
vec_b <- 100:51
vec_c <- vec_a + vec_b

# each element is 101
vec_c

# the length of vectors is fifty. 50 * 101
sum(vec_c)
```

### Magic trick algorithm {-}

Define a variable named `x` that contains an integer value and perform the following operations in sequence:

- Redefine `x` by adding 1.
- Double the resulting number, over-writing `x`.
- Add 4 to `x` and save the result as `x`.
- Redefine `x` as half of the previous value of `x`.
- Subtract the originally chosen arbitrary number from `x`.

Print `x`. Restart the algorithm defined above by choosing a new arbitrary natural number.

```{r}
x <- -999  # arbitrary integer
x_save <- x  # save for the last step
x <- x + 1
x <- x * 2
x <- x + 4
x <- x / 2
x - x_save
```

### Vectors {-}

Print the object `datasets::rivers` and consult the manual of this object.

- What is the class of the object? 
- What is the length of the object?
- Calculate the mean, median, minimum, maximum, and the 33%-quantile across all values. 

```{r}
class(datasets::rivers)
length(datasets::rivers)
mean(datasets::rivers)
# other functions can easily be found on the internet ;-)
```

### Data frames {-}

Print the object `datasets::quakes` and consult the manual of this object.

- Determine the dimensions of the data frame using the respective function in R.
- Extract the vector of values in the data frame that contain information about the Richter Magnitude.
- Determine the value largest value in the vector of event magnitudes.
- Determine the geographic position of the epicenter of the largest event.

```{r}
dim(datasets::quakes)
vec <- datasets::quakes$mag
max(vec)
idx <- which.max(vec)  # index of largest value

# geographic positions defined by longitude and latitude (columns long and lat)
datasets::quakes$long[idx]
datasets::quakes$lat[idx]
```

### Workspace {-}

Create a new *R project* and create sub-directories in a meaningful way (as described in this Chapter). Create an RMarkdown file in your new project which implements your solutions to above exercises. Give the file a title, implement some structure in the document, and write some text explaining what your code does.

*No solutions provided.*

## Programming primers

### Gauss variations {-}

Use a `for` loop to compute the sum of all natural numbers from 1 to 100. Print the result to the screen. Repeat this exercise but use a `while` loop.
```{r}
# 1a. for-loop to compute sum from 1 - 100
sum <- 0
for (i in 1:100){
  sum <- sum + i # for-loop iterating from 1 to 100 
}
print(sum)

# 1b. while-loop to compute sum from 1 - 100
loop_status <- TRUE
counter <- 0
sum <- 0
while (loop_status) { # while-loop is repeated as long as loop_status is true
  counter <- counter + 1
  sum <- sum + counter
  if (counter == 100) loop_status <- FALSE
}
print(sum)
```

Add up all numbers between 1 and 100 that are at the same time a multiple of 3 and a multiple of 7. Print the result to the screen in the form of: `The sum of multiples of 3 and 7 within 1-100 is: {your result}`.
```{r}
sum <- 0
for (i in seq(100)) {
	if (i %% 3 == 0 && i %% 7 == 0 ) {
		sum <- sum + i
	}	
}	
print(paste0("The sum of multiples of 3 and 7 within 1-100 is: ", sum))
```

### Nested loops {-}

Given a matrix `mymat` and a vector `myvec` (see below), implement the following algorithm:

1. Start with the first row in `mymat`.
2. Fill all missing values in the current row of `mymat` with the maximum value in `myvec`.
3. Drop the maximum value from `myvec`.
4. Proceed to the next row of `mymat` and repeat steps 2-4.

`mymat` and `myvec` are defined as:
```{r}
mymat <- matrix(c(6, 7, 3, NA, 15, 6, 7, 
              NA, 9, 12, 6, 11, NA, 3, 
              9, 4, 7, 3, 21, NA, 6, 
              rep(NA, 7)),
            nrow = 4, byrow = TRUE)
myvec <- c(8, 4, 12, 9, 15, 6)
```

```{r message=FALSE}
for (i in 1:nrow(mymat)){
  for (j in 1:ncol(mymat)){
    if (is.na(mymat[i,j])){
      mymat[i,j] <- max(myvec)
    }
  }
  myvec <- myvec[-which.max(myvec)] # update the B vector removing the maximum value
}
mymat
```

### Interpolation {-}

Define a vector $\vec{v}$ of length 100. Define the vector so that $v_i = 6$, for $i = 1 : 25$ and $v_i = -20$, for $i = 66 : 100$. Remaining elements are to be defined as 'missing'. Linearly interpolate missing values that are not defined. Plot the values of $\vec{v}$ using `plot(vec)`.

```{r}
vec <- rep(NA, 100) # initialize vector of length 100 with NA
vec[1:25] <- 6      # populate first 25 elements of 'vec' with 6. 
vec[66:100] <- -20  # populate elements 66:100 with -20.

# Determine index of last non-missing value before gap
last_non_na <- 1
while (!is.na(vec[last_non_na+1])) last_non_na <- last_non_na + 1

# determine index of first non-missing value after gap
first_non_na <- last_non_na + 1
while (is.na(vec[first_non_na])) first_non_na <- first_non_na + 1	

# Get the increment that is needed for interpolation
last_value  <- vec[last_non_na]  # Last non-NA value
first_value <- vec[first_non_na] # First non-NA value
delta <- (last_value - first_value) / (last_non_na - first_non_na) # Change in y over change in x

# fill missing values incrementally
for (i in 2:length(vec)){
  if (is.na(vec[i])) vec[i] <- vec[i-1] + delta
}

plot(vec)

# or short using the approx() function:
vec <- rep(NA, 100) # initialize vector of length 100 with NA
vec[1:25] <- 6      # populate first 25 elements of 'vec' with 6. 
vec[66:100] <- -20  # populate elements 66:100 with -20.

vec <- approx(1:100, vec, xout = 1:100)

plot(vec)
```

## Random Forest

### Fitting a Random Forest {-}

Fit a random forest model to the flux data used in the examples of this chapter. Implement bagging 12 decision trees (`num.trees`), each with a maximum depth of 4 (`max.depth`). You can consult the respective arguments for the `"ranger"` method typing `?ranger`. 

```{r}
# Data loading and cleaning
daily_fluxes <- read_csv("./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") |>  
  
  # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  dplyr::na_if(-9999) |>
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))

# Data splitting
set.seed(123)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# The same model formulation is in the previous chapter
pp <- recipes::recipe(GPP_NT_VUT_REF ~ TA_F + SW_IN_F + LW_IN_F + VPD_F + P_F + WS_F, 
                      data = daily_fluxes_train) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

mod1 <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  tuneGrid = expand.grid( .mtry = floor(6 / 3),        # default p/3
                        .min.node.size = 5,            # default 5
                        .splitrule = "variance"),      # default "variance"
  # arguments specific to "ranger" method
  replace = FALSE,
  max.depth = 4,
  sample.fraction = 0.5,
  num.trees = 12,       
  seed = 1982                                          # for reproducibility
)

# generic print
print(mod1)
```

Repeat the fitting with 1000 decision trees and max depth of 4, then with 12 decision trees and a max depth of 6. Then, discuss the role that the number of decision trees and the maximum depth of a tree play in the bias-variance trade-off and in the computation time. 

```{r}
# Directly fit the model again, with same data and model formulation
# this time num.trees = 1000
mod2 <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  tuneGrid = expand.grid( .mtry = floor(6 / 3),        # default p/3
                        .min.node.size = 5,            # default 5
                        .splitrule = "variance"),      # default "variance"
  # arguments specific to "ranger" method
  replace = FALSE,
  max.depth = 4,
  sample.fraction = 0.5,
  num.trees = 1000,       
  seed = 1982                                          # for reproducibility
)

# generic print
print(mod2)
```

```{r}
# Repeat model fit with max.depth = 6
mod3 <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  tuneGrid = expand.grid( .mtry = floor(6 / 3),        # default p/3
                        .min.node.size = 5,            # default 5
                        .splitrule = "variance"),      # default "variance"
  # arguments specific to "ranger" method
  replace = FALSE,
  max.depth = 6,
  sample.fraction = 0.5,
  num.trees = 12,       
  seed = 1982                                          # for reproducibility
)

# generic print
print(mod3)
```

** Interpretation of results**

```{r}
# check results
rbind(
  mod1$results,
  mod2$results,
  mod3$results
)[,- c(1:3)]
```

Increasing the number of decision trees leads to a decrease in error (e.g. RMSE) and in error variance in the random forest results, making the model more accurate, robust and generalisable. This can be seen by the smaller values of practically all the metrics above. In the bias-variance trade-off, higher `num.trees` shifts the balance towards both lower bias and lower variance.

Increasing the number of variables considered for each split allows for a better fit of the data, but makes the model less generalisable. This is seen by a reduction in the error metrics but an increase in their estimated standard deviation. Hence, using high values for `mtry` may lead to overfitting.

### Hyperparameter tuning {-}

In a previous tutorial, you learned how to tune the hyperparameter $k$ in a KNN by hand. Now you will do the hyperparameter tuning for a Random Forest model. The task gets more complicated because there are more hyperparameters in a random forest. The {caret} package allows to vary three hyperparameters:

-   `mtry`: The number of variables to consider to make decisions at each node, often taken as $p/3$ for regression, where $p$ is the number of predictors.
-   `min.node.size`: The number of data points at the "bottom" of each decision tree, i.e. the leaves.
-   `splitrule`: The function applied to data in each branch of a tree, used for determining the goodness of a decision.

Answer the following questions, giving a reason for your responses:

1.   Check the help for the `ranger()` function and identify which values each of the three hyperparameters/arguments can take. Select a sensible range of values for each hyperparameter, that you will use in the hyperparameter search.

```{r}
mtry_values <- c(2, 4, 6)
min.node.size_values <- c(2, 5, 10, 20)
splitrule_values <- c("variance", "extratrees", "maxstat")
```

2.   In the previous exercise, you have seen how the maximum tree depth regulates fit quality and overfitting. How does the minimum node size relate to tree depth? What happens at the edge cases, when `min.node.size = 1` and when `min.node.size = n` (`n` being the number of observations)? For the next questions, it's not necessary to provide the `max.depth` argument to `train()` because `min.node.size` is already limiting the size of the trees in the random forests.

**Solution**:
The minimum node size is inversely related to the tree depth. The more nodes are required to be at the leaves, the shorter the tree will be, because not so many splits can be done. If we allowed each leave to correspond to a single observation, we would have a very large tree, such that each branch corresponds to one observation. If we force each leave to have at least $n$ observations, then the tree will not "grow", that is, it will never even split.

3.   *Greedy hyperparameter tuning*: Sequentially optimize the choice of each hyperparameter, one at a time and keeping the other two constant. Take the code from the tutorial as a starting point, and those hyperparameter values as an initial point for the search.
> Tip: Keep the number of trees low, otherwise it takes too long to fit each random forest model.

```{r}
# Use data and model formulation created before

results <- c()   # initialise results
set.seed(1997)   # for reproducibility

# Train models in a loop, save metrics
for(mtry_value in mtry_values){
  mod <- caret::train(
    pp, 
    data = daily_fluxes_train %>% 
      drop_na(), 
    method = "ranger",
    metric = "RMSE",
    trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
    tuneGrid = expand.grid( .mtry = mtry_value,           # modify mtry
                          .min.node.size = 5,            # default 5
                          .splitrule = "variance"),      # default "variance"
    # arguments specific to "ranger" method
    replace = FALSE,
    sample.fraction = 0.5,
    num.trees = 100,                                     # keep small for computation
    seed = 1982                                          # for reproducibility
  )
  
  results <- rbind(results, mod$results)
}

results
```

Based on the first round of hyperparameter tuning, we should take `mtry = 2` because it leads to the smallest RMSE and MAE, and the highest Rsquared. That is, the best fit. Nevertheless, the difference between taking `mtry = 2` or `4` is very small and the second actually leads to smaller variance in the metric estimates. Hence, the decision is not so clear.

```{r}
# Take the previous best model and tune next hyperparameter

results <- c()   # initialise results
set.seed(1997)   # for reproducibility

# Train models in a loop, save metrics
for(min.node.size_value in min.node.size_values){
  mod <- caret::train(
    pp, 
    data = daily_fluxes_train %>% 
      drop_na(), 
    method = "ranger",
    metric = "RMSE",
    trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
    tuneGrid = expand.grid( .mtry = 2,      # best mtry
                          .min.node.size = min.node.size_value,   # modify
                          .splitrule = "variance"),      # default "variance"
    # arguments specific to "ranger" method
    replace = FALSE,
    sample.fraction = 0.5,
    num.trees = 100,                                     # keep small for computation
    seed = 1982                                          # for reproducibility
  )
  
  results <- rbind(results, mod$results)
}

results
```

The second hyperparameter should be either `min.node.size = 2` or `5` because they lead to the best metrics overall. Again, the differences are very small and each metric would lead to a different decision. By increasing `min.node.size`, we get more generalisability, but if it's too high we will lose fit quality. If you change the random seed, you'll see that the tuning results are not robust, so whichever hyperparameter we choose won't make a big difference in the model fit (at least within the ranges searched). Let's take `min.node.size = 5` for the next loop.

```{r}
# Take the previous best models and tune last hyperparameter
results <- c()

# Train models in a loop, save metrics
for(splitrule_value in splitrule_values){
  mod <- caret::train(
    pp, 
    data = daily_fluxes_train %>% 
      drop_na(), 
    method = "ranger",
    metric = "RMSE",
    trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
    tuneGrid = expand.grid( .mtry = 4,                    # best mtry
                          .min.node.size = 5,             # best min.node.size
                          .splitrule = splitrule_value),  # modify
    # arguments specific to "ranger" method
    replace = FALSE,
    sample.fraction = 0.5,
    num.trees = 100,                                     # keep small for computation
    seed = 1982                                          # for reproducibility
  )
  
  results <- rbind(results, mod$results)
}

results
```

According to the last round of tuning, we should use `spliturle = "extratrees"`. With that, we found the best model so far.

4.   *Grid hyperparameter tuning*: Starting with the same range of values for each hyperparameter as before, look for the combination that leads to the best model performance among all combinations of hyperparameter values. This time, use the `expand.grid()` function to create a data.frame of hyperparameter value combinations. This grid will be passed to `train()` via the `tuneGrid` argument (see example in the tutorial). This will automatically do the hyperparameter search for you. Comment the output of `train()` and the results of the hyperparameter search.

```{r}
set.seed(1403)    # for reproducibility

mod <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  # expand grid of tunable hyperparameters
  tuneGrid = expand.grid( .mtry = mtry_values,             
                        .min.node.size = min.node.size_values,    
                        .splitrule = splitrule_values),  
  # arguments specific to "ranger" method
  replace = FALSE,
  sample.fraction = 0.5,
  num.trees = 100,                                     # keep small for computation
  seed = 1982                                          # for reproducibility
)

plot(mod, metric = "RMSE")
plot(mod, metric = "Rsquared")
```

5.   Compare the results from the two hyperparameter tuning approaches. Do the optimal hyperparameters coincide? Are the corresponding RMSE estimates similar? What are the advantages and disadvantages of the greedy and the grid approaches?

The best model according to the grid search, with lowest RMSE, is the one with `mtry = 4`, `min.node.size = 5` and `splitrule = "extratrees"` . This is not the "best model" we found with the greedy approach (different `mtry`) and also not the best according to Rsquared (`mtry = 4`, `min.node.size = ` and `splitrule = ""`). The metric used for tuning matters, and looking at several of them at the same can help make decisions, if different metrics agree. All these hyperparameter tuning approaches agree that the best `splitrule` is `"extratrees"` (and if you change the random seed, this result is consistent). The best `mtry` value is different for each split rule used, so having started with `"variance"` in the greedy search lead the tuning in the wrong direction. This highlights that hyperparameter values interact with each other and optimizing over grids is preferred (although it takes more time).

### Model performance {-}

You have trained several random forest models. Evaluate the model performance on the best model (the one for the tuned hyperparameters) and on one of your worse models. If you compare the RMSE and $R^2$ on the training and the test set, does it show overfitting?

```{r}
# Train best model
mod_best <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  # expand grid of tunable hyperparameters
  tuneGrid = expand.grid( .mtry = 6,             
                        .min.node.size = 10,    
                        .splitrule = "extratrees"),  
  # arguments specific to "ranger" method
  replace = FALSE,
  sample.fraction = 0.5,
  num.trees = 100,                                     # keep small for computation
  seed = 1982                                          # for reproducibility
)

# Get predictions

# Train worst model
mod_worst <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  # expand grid of tunable hyperparameters
  tuneGrid = expand.grid( .mtry = 2,             
                        .min.node.size = 20,    
                        .splitrule = "maxstat"),  
  # arguments specific to "ranger" method
  replace = FALSE,
  sample.fraction = 0.5,
  num.trees = 100,                                     # keep small for computation
  seed = 1982                                          # for reproducibility
)

source("R/eval_model.R")
eval_model(mod_best, daily_fluxes_train, daily_fluxes_test)
eval_model(mod_worst, daily_fluxes_train, daily_fluxes_test)
```

The performance on the test set for the best model is still close to the performance on the training set, so the model doesn't seem to overfit. The same goes for the worse model, which leads to worse $R^2$ and RMSE and visually the fit is slightly worse.