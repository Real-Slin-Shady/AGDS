# (APPENDIX) Appendix {-}

# Solutions {#solutions}

## Getting Started

### Dimensions of a circle {-}

- Given the radius of a circle write a few lines of code that calculates its area and its circumference. Run your code with different values assigned of the radius. 
```{r}
radius <- 1
area <- pi * radius^2
circum <- 2 * pi * radius
```

- Print the solution as text.
```{r}
print(paste("Radius:", radius, "   Circumference:", circum))
```

### Sequence of numbers {-}

Generate a sequence of numbers from 0 and $\pi$ as a vector with length 5.
```{r}
seq(0, pi, length.out = 5)
```

### Gauss sum {-}

Rumors have it that young Carl Friedrich Gauss was asked in primary school to calculate the sum of all natural numbers between 1 and 100. He did it in his head in no time. We're very likely not as intelligent as young Gauss. But we have R. What's the solution?

```{r}
sum(1:100)
```

Gauss calculated the sum with a trick. The sum of 100 and 1 is 101. The sum of 99 and 2 is 101. You do this 50 times, and you get $50 \times 101$. Demonstrate Gauss' trick with vectors in R.
```{r}
vec_a <- 1:50
vec_b <- 100:51
vec_c <- vec_a + vec_b

# each element is 101
vec_c

# the length of vectors is fifty. 50 * 101
sum(vec_c)
```

### Magic trick algorithm {-}

Define a variable named `x` that contains an integer value and perform the following operations in sequence:

- Redefine `x` by adding 1.
- Double the resulting number, over-writing `x`.
- Add 4 to `x` and save the result as `x`.
- Redefine `x` as half of the previous value of `x`.
- Subtract the originally chosen arbitrary number from `x`.

Print `x`. Restart the algorithm defined above by choosing a new arbitrary natural number.

```{r}
x <- -999  # arbitrary integer
x_save <- x  # save for the last step
x <- x + 1
x <- x * 2
x <- x + 4
x <- x / 2
x - x_save
```

### Vectors {-}

Print the object `datasets::rivers` and consult the manual of this object.

- What is the class of the object? 
- What is the length of the object?
- Calculate the mean, median, minimum, maximum, and the 33%-quantile across all values. 

```{r}
class(datasets::rivers)
length(datasets::rivers)
mean(datasets::rivers)
quantile(datasets::rivers, probs = 0.33)
```

### Data frames {-}

Print the object `datasets::quakes` and consult the manual of this object.

- Determine the dimensions of the data frame using the respective function in R.
- Extract the vector of values in the data frame that contain information about the Richter Magnitude.
- Determine the value largest value in the vector of event magnitudes.
- Determine the geographic position of the epicenter of the largest event.

```{r}
dim(datasets::quakes)
vec <- datasets::quakes$mag
max(vec)
idx <- which.max(vec)  # index of largest value

# geographic positions defined by longitude and latitude (columns long and lat)
datasets::quakes$long[idx]
datasets::quakes$lat[idx]
```

### Workspace {-}

Create a new *R project* and create sub-directories in a meaningful way (as described in this Chapter). Create an RMarkdown file in your new project which implements your solutions to above exercises. Give the file a title, implement some structure in the document, and write some text explaining what your code does.

*No solutions provided because part of the final report.*

## Programming primers

### Gauss variations {-}

```{r}
# for-loop to compute sum from 1 - 100
sum <- 0
for (i in 1:100){
  sum <- sum + i # for-loop iterating from 1 to 100 
}
print(sum)
```


```{r}
# while-loop to compute sum from 1 - 100
loop_status <- TRUE
counter <- 0
sum <- 0
while (loop_status) { # while-loop is repeated as long as loop_status is true
  counter <- counter + 1
  sum <- sum + counter
  if (counter == 100) loop_status <- FALSE
}
print(sum)
```

```{r}
# Initiate sum variable
sum <- 0

# Go through loop from 1 to 100
for (i in seq(100)) {
  
  # Check if the current number a muliple of three and seven
  # The modulo operator '%%' returns the remainder of a division
	if (i %% 3 == 0 && i %% 7 == 0 ) {
		sum <- sum + i
	}	
}	
print(paste0("The sum of multiples of 3 and 7 within 1-100 is: ", sum))
```

### Nested loops {-}

```{r}
mymat <- matrix(c(6, 7, 3, NA, 15, 6, 7, 
              NA, 9, 12, 6, 11, NA, 3, 
              9, 4, 7, 3, 21, NA, 6, 
              rep(NA, 7)),
            nrow = 4, byrow = TRUE)
myvec <- c(8, 4, 12, 9, 15, 6)
```

```{r message=FALSE}
# Loop over the rows in `mymat`.
for (i in 1:nrow(mymat)){
  
  # Loop over the columns in `mymat`.
  for (j in 1:ncol(mymat)){
    
    # Check if current value is missing, if so overwrite with max in 'myvec'
    if (is.na(mymat[i,j])){
      mymat[i,j] <- max(myvec)
    }
  }
  myvec <- myvec[-which.max(myvec)] # update the vector removing the maximum value
}
mymat
```

### Interpolation {-}

```{r}
# Set up vector as required in the exercise
vec <- rep(NA, 100) # initialize vector of length 100 with NA
vec[1:25] <- 6      # populate first 25 elements of 'vec' with 6. 
vec[66:100] <- -20  # populate elements 66:100 with -20.

# Determine index of last non-missing value before gap
last_non_na <- 1
while (!is.na(vec[last_non_na+1])) last_non_na <- last_non_na + 1

# determine index of first non-missing value after gap
first_non_na <- last_non_na + 1
while (is.na(vec[first_non_na])) first_non_na <- first_non_na + 1	

# Get the increment that is needed for interpolation
last_value  <- vec[last_non_na]  # Last non-NA value
first_value <- vec[first_non_na] # First non-NA value
delta <- (last_value - first_value) / (last_non_na - first_non_na) # Change in y over change in x

# fill missing values incrementally
for (i in 2:length(vec)){
  if (is.na(vec[i])) vec[i] <- vec[i-1] + delta
}

plot(vec)

# or short using the approx() function:
vec <- rep(NA, 100) # initialize vector of length 100 with NA
vec[1:25] <- 6      # populate first 25 elements of 'vec' with 6. 
vec[66:100] <- -20  # populate elements 66:100 with -20.

vec <- approx(1:100, vec, xout = 1:100)

plot(vec)
```

## Data wrangling

### Star wars {-}

{dplyr} comes with a toy dataset `dplyr::starwars` (just type it into the console to see its content). Have a look at the dataset with `View()`. Play around with the dataset to get familiar with the {tidyverse} coding style. Use (possibly among others) the functions `dplyr::filter()`, `dplyr::arrange()`, `dplyr::pull()`, `dplyr::select()`, `dplyr::desc()` and `dplyr::slice()` to answer the following question:

- How many pale characters come from the planets Ryloth or Naboo?
```{r message=FALSE}
dplyr::starwars |> 
  dplyr::filter(skin_color == "pale" & (homeworld == "Naboo" | homeworld == "Ryloth")) |> 
  nrow()   
```

- Who is the oldest among the tallest thirty characters?
```{r message=FALSE}
dplyr::starwars |> 
  arrange(desc(height)) |> 
  slice(1:30) |> 
  arrange(birth_year) |> 
  slice(1) |> 
  pull(name)
```

- What is the name of the shortest character and their starship in "Return of the Jedi"?
```{r message=FALSE}
dplyr::starwars |> 
  unnest(films) |> 
  filter(films == "Return of the Jedi") |> 
  unnest(starships) |> 
  arrange(height) |> 
  slice(1) |> 
  select(name, starships)
```

### Aggregating {-}

You have learned about aggregating in the {tidyverse}. Let's put it in practice.

- Reuse the code in the tutorial to read, reduce, and aggregate the `half_hourly_fluxes` dataset to the daily scale, calculating the following metrics across half-hourly `VPD_F` values within each day: mean, 25% quantile, and 75% quantile. 
```{r message=FALSE, warning=FALSE}
half_hourly_fluxes <- read_csv("data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv") |>
  
  # Select only variables that we are interested in
  dplyr::select(
    starts_with("TIMESTAMP"),
    ends_with("_F"),
    GPP_NT_VUT_REF,
    NEE_VUT_REF_QC,
    -starts_with("SWC_F_MDS_"),
    -contains("JSB")
  )

daily_fluxes <- half_hourly_fluxes |>  

  # Clean the datetime objects
  dplyr::mutate(date_time = lubridate::ymd_hm(TIMESTAMP_START),
                date      = lubridate::date(date_time)) |>

  # Aggregate to daily scale
  dplyr::group_by(date) |> 
  dplyr::summarise(mean = mean(VPD_F),
                   q25  = quantile(VPD_F, probs = 0.25),
                   q75  = quantile(VPD_F, probs = 0.75)
                   )
```

- Retain only the daily data for which the daily mean VPD is among the upper or the lower 10% quantiles.
```{r}
# In two steps. First, get thresholds of the quantiles
thresholds <- quantile(daily_fluxes$mean, probs = c(0.1, 0.9))

# Then, filter data to be above/below the upper/lower quantiles and combine
daily_fluxes_sub <- daily_fluxes |> 
  
  # in lower 10% quantile
  filter(mean < thresholds[1]) |> 
  mutate(qq = "lower") |>   # add label
  
  # combine
  bind_rows(
    
    daily_fluxes |> 
      # in upper 90% quantile
      filter(mean > thresholds[2]) |> 
      mutate(qq = "upper")
    )
```

- Calculate the mean of the 25% and the mean of the 75% quantiles of half-hourly VPD within the upper and lower 10% quantiles of mean daily VPD.
```{r}
daily_fluxes_sub |> 
  group_by(qq) |> 
  summarise(q25_mean = mean(q25),
            q75_mean = mean(q75))
```


### Patterns in data quality {-}

The uncleaned dataset `FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv` holds half-hourly data that is sometimes of poor quality. Investigate whether NEE data quality is randomly spread across hours in a day by calculating the proportion of (i) actually measured data, (ii) good quality gap-filled data, (iii) medium quality data, and (iv) poor quality data within each hour-of-day (24 hours per day).
```{r message=FALSE, warning=FALSE}
# using half_hourly_fluxes read above
daily_fluxes <- half_hourly_fluxes |>
  mutate(TIMESTAMP_START = lubridate::ymd_hm(TIMESTAMP_START)) |> 
  mutate(hour_of_day = lubridate::hour(TIMESTAMP_START)) |> 
  group_by(hour_of_day) |> 
  summarise(n_measured = sum(NEE_VUT_REF_QC == 0),
            n_good     = sum(NEE_VUT_REF_QC == 1),
            n_medium   = sum(NEE_VUT_REF_QC == 2),
            n_poor     = sum(NEE_VUT_REF_QC == 3),
            n_total    = n()
            ) |> 
  mutate(f_measured = n_measured / n_total,
         f_good     = n_good     / n_total,
         f_medium   = n_medium   / n_total,
         f_poor     = n_poor     / n_total,
         )
```


Interpret your findings: Are the proportions evenly spread across hours in a day? 
```{r}
# this is not asked for but interesting. More on data visualisation in Chapter 5
daily_fluxes |> 
  pivot_longer(c(f_measured, f_good, f_medium, f_poor), 
               names_to = "quality", 
               values_to = "fraction") |> 
  ggplot(aes(x = hour_of_day, y = fraction, color = quality)) +
  geom_line()

# you can also just look at values of df$f_measured over the course of a day (hod)  
```

Perform an aggregation of the half-hourly GPP data (variable `GPP_NT_VUT_REF`) to daily means of the unmodified data read from file `FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv`, and from cleaned data where only measured (not gap-filled) half-hourly data is kept and aggregated. This yields two data frames with daily GPP data. Calculate the overall mean GPP for the two data frames (across all days in the data frame). Are the overall mean GPP values equal? If not, why?
```{r message=FALSE, warning=FALSE}
daily_fluxes_all <- half_hourly_fluxes |>
  dplyr::mutate(date_time = lubridate::ymd_hm(TIMESTAMP_START),
                date      = lubridate::date(date_time)) |>
  dplyr::group_by(date) |> 
  summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE))

daily_fluxes_cleaned <- half_hourly_fluxes |>
  dplyr::mutate(date_time = lubridate::ymd_hm(TIMESTAMP_START),
                date      = lubridate::date(date_time)) |>
  mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC == 0, GPP_NT_VUT_REF, NA)) |> 
  dplyr::group_by(date) |> 
  summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE))

# overall means
daily_fluxes_all |> 
  summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE))

daily_fluxes_cleaned |> 
  summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE))
```

### Tidying an Excel-file {-}

*No solutions provided because part of the final report*

## Data Visualisation

### Identifying Outliers {-}

```{r message=FALSE}
# Read and wrangle data
half_hourly_fluxes <- readr::read_csv("data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv") |> 
  
    # interpret -9999 as missing value
    dplyr::na_if(-9999) |> 
  
    # interpret all variables starting with TIMESTAMP as a date-time object
    dplyr::mutate_at(vars(starts_with("TIMESTAMP_")), lubridate::ymd_hm)

# aggregate to daily
daily_fluxes <- half_hourly_fluxes |> 
  
    # create a date-only object
    dplyr::mutate(date = lubridate::as_date(TIMESTAMP_START)) |> 
  
    # aggregate
    dplyr::group_by(date) |> 
    dplyr::summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE),
              PPFD_IN = mean(PPFD_IN, na.rm = TRUE),
              .groups = 'drop' # not mandatory
    )
```


```{r}
# fit linear regression
linmod <- lm(GPP_NT_VUT_REF ~ PPFD_IN, data = daily_fluxes)
linmod
```


```{r}
# get box plot statistics for determining "outlying" points
out_boxplot <- boxplot.stats(linmod$residuals)

# record the row numbers of outlying points based on the output list element 'out'
# row numbers are the names of elements in out_boxplot$out, provided as strings. 
# convert them to integers.
outlier_index <- out_boxplot$out |> names() |> as.integer()
```


```{r}
# Label outliers

# In base-R, this could be done as
daily_fluxes$is_outlier <- FALSE
daily_fluxes$is_outlier[outlier_index] <- TRUE

# In tidyverse style:
daily_fluxes <- daily_fluxes |> 
  dplyr::mutate(rownumber = dplyr::row_number()) |>    # could also do: dplyr::mutate(rownumber = 1:nrow(.))
  dplyr::mutate(is_outlier = ifelse(rownumber %in% outlier_index, TRUE, FALSE))
```


```{r}
# Create scatterplot
daily_fluxes |> 
  tidyr::drop_na() |>
  ggplot() +
  
  # Set variables to use for axes and filling of plots
  aes(x = PPFD_IN,
      y = GPP_NT_VUT_REF,
      fill = is_outlier) +
  geom_point(size  = 2,  # Increase size of points
             shape = 21) + # Shape 21 has a "fill" and "color" property
                           # fill refers to the shape itself
                           # color refers to the border around the shape
  theme_classic() +        # Pick a simple theme
  scale_fill_brewer(        # Pick a nice color scheme
    "Outlier?",             # Set title of legend
    palette = "Dark2",      # Pick a nice palette of the scheme
    labels = c("No", "Yes") # Add labels to the legend
  ) +
  labs(title = "Outliers in GPP ~ PPFD",
       y    = expression("Gross primary production [gC m"^-2 * "s"^-1 * "]"),
       x    = expression("Photosynthetic photon flux density [" * mu * "mol" ~ m^-2 ~ s^-1 * "]"))
```

**What do we see in this plot?** We see that the orange points, that is the outliers, actually fall outside the main point cloud of green points. The distribution of these outliers seems without systematic pattern or deviation. Nonetheless, it is good practice to go a step further and look at these data points in detail to find out whether they should be removed or not in your analysis. In later Chapters you will learn more on what disproportionate role outliers can play and how they may affect your statistical model and analysis.

### Diurnal and seasonal cycles {-}
```{r message=FALSE, warning=FALSE}
# Data wrangling
fluxes_per_hod_doy <-
  half_hourly_fluxes |>                        # df from previous exercise
  dplyr::mutate(
    hour_day = lubridate::hour(TIMESTAMP_START),     # Save the hour of the day
    day_year = lubridate::yday(TIMESTAMP_START)) |>  # Save the day of the year
  dplyr::group_by(hour_day, day_year) |>              # Group by HoD and DoY at once
  dplyr::summarise(gpp = mean(GPP_NT_VUT_REF, na.rm = TRUE)) # Take mean for GPP, ignore `NA`
```


```{r}
# Create a standard raster plot
fluxes_per_hod_doy |> 
  ggplot() +
  aes(x = hour_day, y = day_year, fill = gpp) +
  geom_raster()
```


```{r}
# Make raster plot publishable - This is up to your judgement. Below is an example. 

fluxes_per_hod_doy |> 
  
  # Specify aesthetics
  ggplot(aes(x = hour_day, 
             y = day_year, 
             fill = gpp)) + # The 'filling' of the raster
  geom_raster() +
  
  # Use a color scale that works also for color-blind people
  scale_fill_viridis_c(option = "magma") +
  
  # Change theme and layout a bit 
  theme(
    plot.title = element_text(face = "bold", # Make title bold
                              hjust = 0.5),  # Horizontal adjustment of title to
                                             # be in the middle (left: 0, right: 1)
    legend.position = "right"               # Move legend to bottom
  ) +
  
  # adjust the aspect ratio of the plotting region
  coord_fixed(ratio=0.075) +
  
  # labels of each mapping axis, \n is a line break
  labs(title = "Diurnal and seasonal cycle of GPP",
       x = "Hour of day", 
       y = "Day of year", 
       fill = expression("GPP \n[gC m"^-2 * "s"^-1 * "]: ")) + # Legend title for fill scale
                                                               # The '\n' adds a line break
  
  # avoid having a small padding from the lowest values to the end of axes
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0))
```

Beautiful, isn't it? We  nicely see that on practically all days of the year we have the diurnal cycle of GPP which follows the sun's cycle. And the throughout a year, we see a rapid increase in GPP in the spring when all trees put out their leaves at once. After a highly productive summer, temperatures drop, sensescence kicks in and GPP gradually drops into its winter low.

### Trend in carbon dioxide concentrations {-}

```{r}
# Read table, set 'header = TRUE' to tell R that the first row are variable names 
# You could set 'skip = 70' to skip all metadata, but R detects that automatically
ml_co2 <- read.table("data/co2_monthly_maunaloa.txt", 
                     header = TRUE,
                     na.strings = '-99.99' # The dataset specifys that these are missing variables
                     ) |>
  
  # To get a good timeseries, we need to get a nice date-time variable first
  dplyr::mutate(date = paste(yr, mo, sep = "-") |> lubridate::ym(), # Imprecise but okay
         datetime = lubridate::date_decimal(decyr, tz = "UTC")) # Exact second of measurement
```

```{r}
# Simple plot of the trend
ml_co2 |> ggplot() + geom_line(aes(datetime, co2_avg))
```

The data looks reasonable. No outliers, no reamining `-99.99` values that where not dropped.

```{r}
# Write the running mean function
running_mean <- function(input_data, # Input vector
                         box_size    # Number of elements over which to average
                         ){
  
  # Create an empty vector of the same length as the input data
  output_data <- rep(NA, length(input_data))
  
  # Loop over each position in the vector
  for (i in 1:length(input_data)){
    
    # Define start and end of the box to average over
		startbox <- i - box_size / 2
		endbox   <- i + box_size / 2
		
		# Do not take means at the border of the input_data
		if (startbox < 1) {
			output_data[i] <- NA	
		} else if (endbox > length(input_data)) {
			output_data[i] <- NA
		} else {
		  # Take the mean of values within the box
			output_data[i] <- mean(input_data[startbox:endbox],
			                       na.rm = TRUE)
		}	
  }
  
  # Return output_data
  return(output_data)
}

ml_co2$co2_rm <- running_mean(ml_co2$co2_avg, 12)
```

```{r}
# Create a nice plot
ml_co2 |>
  ggplot() +
  
  # First plot the measurements
  geom_line(aes(datetime, co2_avg, color = "Measured")) +
  # Add running mean on top
  geom_line(aes(datetime, co2_rm, color = "Running Mean")) +
  # Style the plot
  theme_classic() +
  theme(legend.position = c(0.75, 0.25)) + # Move legend into the plot (c(x, y))
  scale_color_manual(
    "", # Omit legend title
    values = c("black", "tomato"),
    labels = c("Measured", "Running Mean")
  ) +
  labs(title = "Carbon dioxide concentrations at Manua Lao",
       y     = expression("CO"[2] ~ "[ppm]"),
       x     = "Year")
```

### Telling a story from data {-}

*No solutions provided because part of the final report.*

## Data Variety

### Files and file formats {-}

#### Reading and writing human readable files {.unnumbered}

The below code shows how to read in the different demo data sets (CSV files). You will note that they all need separate settings, and that a given file extension isn't necessarily a reflection of the content the file. Inspection of your read in data is therefore key.

```{r}
# read in the first demo
demo_01 <- read.table(
  "https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_1.csv",
  sep = ",",
  header = TRUE
  )

# read in second demo
demo_02 <- read.table(
  "https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_2.csv",
  sep = " ",
  header = TRUE
  )

demo_03 <- read.table(
  "https://raw.githubusercontent.com/geco-bern/agds/main/data/demo_3.csv",
  sep = ";",
  comment.char = "|",
  header = TRUE,
  )
```

All the demo data sets are equal, except for their formatting. We can test if the content is identical by using the `identical()` function in R.

```{r}
# compare 1 with 2
identical(demo_01, demo_02)

# compare 2 with 3
identical(demo_02, demo_03)

# Given transitive properties, demo_01 is identical to demo_03
```

You can combine the three datasets using the {dplyr} `bind_rows()` function.
```{r}
# combining all demo datasets
demo_all <- dplyr::bind_rows(demo_01, demo_02, demo_03)

# writing the data to a temporary CSV file
write.table(
  demo_all, 
  file = file.path(tempdir(), "tmp_csv_file.csv"),
  col.names = TRUE,
  row.names = FALSE,
  sep = ","
)

# or...
write.csv(
  demo_all,
  file.path(tempdir(), "tmp_csv_file.csv"),
  row.names = FALSE
)

# read in the previous CSV file
demo_all_new <-read.table(
  file.path(tempdir(), "tmp_csv_file.csv"),
  header = TRUE,
  sep = ","
)

# writing the data to a JSON file
jsonlite::write_json(demo_all_new, path = "./my_json_file.json")
```

#### Reading and writing binary files {.unnumbered}

1. It is a NetCDF file
2. the {terra} library (see Table xyz)
3. Temperature data for Switzerland at noon on 2021-01-01

```{r eval=FALSE}
# read unknown netcdf file using the {terra} library
library(terra)
unkown_netcdf <- terra::rast("https://raw.githubusercontent.com/geco-bern/agds/main/data/some_data.nc")

# print the meta-data by calling the variable
unknown_netcdf

# visually plot the data
terra::plot(unknown_netcdf)
```

5. 

```{r eval=FALSE}
# write the data as a geotiff (other options are possible as well in writeRaster)
terra::writeRaster(
  unknown_netcdf,
  filename = "./test.tif",
  overwrite = TRUE
  )
```

```{r eval=FALSE}
# read unknown tif file using the {terra} library
library(terra)
unkown_tif <- terra::rast("https://raw.githubusercontent.com/geco-bern/agds/main/data/some_data.tif")

# print the meta-data by calling the variable
unknown_tif

# visually plot the data
terra::plot(unknown_tif)

# Are they exactly the same
terra::plot(unknown_tif - unknown_netcdf)

# or...
identical(unkown_netcdf, unkown_tif)
```

Looks similar to the NetCDF data, however one pixel is different as shown by taking the difference between both datasets.

## Open Science

### External data {-}

The project data is stored in one folder without folders to sort data from code to give it structure.

The project can be re-organized using a simple project structure as such:

```
~/project/
├─ data/
     ├─ 00_convert_data.R
     ├─ survey.xlsx # the original
     ├─ survey.csv # from (xls conversion (copy 1).csv)
├─ R/
     ├─ my_functions.R
├─ analysis/
     ├─ 00_model_fits.R # from Model-test-final.R
     ├─ 01_model_plots.R # from Plots.R
├─ vignettes/
     ├─ Report.Rmd
├─ manuscript/
     ├─ Report.html
     ├─ Figure 1.png
```

Note that duplicate files are removed, code to cleanup data is numbered and stored with the data, functions which are accessible for analysis are stored in the R folder, Rmarkdown files are stored in the vignettes folder and the results of the full analysis is stored in a manuscript folder. Some variations on naming is possible.

### A new project {-}

*No solutions provided.* Note that this exercise trains your ability to access and wrangle data yourself in a reproducible way. The best solution to test whether your you successfully did to is by letting a friend run all of your code on their machine. Resolving the errors you may encounter, helps you to improve you workflow and ensures a streamlined submission of your final report.

### Tracking the state of your project {-}

*No solutions provided.*


## Code Management

### Location based code management {-}

*No solutions provided.*

### Collaborative Work on Github {-}

*No solutions provided because part of the final report.*

## Regression and Classification

### Stepwise regression and asking for help {-}

*No solutions provided because part of the final report.*

## Supervised ML I


### Comparison of LM and KNN {-}

*No solutions provided because part of the final report.*

### The role of picking a hyperparameter {-}

*No solutions provided because part of the final report.*

## Supervised ML II

### Cross-validation by hand {-}

The goal of this exercise is to create a script that explicitly runs a cross-validation by hand. In Chapter \@ref(supervisedmlii), we used the function `caret::train()` and specified the argument `trControl = caret::trainControl(method = "cv", number = 10)` to let it conduct a 10-fold cross-validation under the hood. Now, we want to actually understand this process in detail by reproducing Figure \@ref(fig:kfoldcv) in code. 

#### Source necessary functions  {.unnumbered}
```{r, eval=FALSE}
source("R/eval_model.R")
```

#### Get data and clean it {.unnumbered}
```{r eval=FALSE, message=FALSE}
# Load daily dataframe from Davos
df_org <- readr::read_csv("data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") |>  

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))
```

#### Create training and test sets {.unnumbered}
```{r, eval=FALSE}
# Data splitting
set.seed(123)  # for reproducibility
split      <- rsample::initial_split(df_org, prop = 0.7, strata = "VPD_F")
df_train   <- rsample::training(split)
df_test    <- rsample::testing(split)
```

#### Write functions to facilitate repeated cross-validation {.unnumbered}
```{r eval=FALSE}
## Function to run cross validation
cross_validation_by_hand <- function(df_train,
                                     n_folds,
                                     vec_k
) {

  # Pre-process data
  
  # The function does not know if we are sampling from a randomly distributed 
  # training set! For example, our df_train is structured in time, causing data
  # leakage. Thus, we first have to randomize the df_train that is fed into
  # the function.
  
  rand <- sample(nrow(df_train)) # Get a vector of random indeces
  df_rnd <- df_train[rand, ] # Reorder df_train using random indeces
  
  # Prepare variables used in the loops
  fold_size <- nrow(df_rnd) / n_folds # Specify size of each df in each fold
  df_cv <- list() # Create empty dataframe to capture model performance per fold
    
  # Loop over k's
  for (k in vec_k) {
    
    # Create a loop that goes through each fold
    for (i in 1:n_folds) {
  
      # Get current fold's starting and ending position in the training set 
      index_start <- (i - 1) * fold_size + 1
      index_end   <- (i) * fold_size
      
      # Split test set into training and validation set
      i_df_val   <- df_rnd |> dplyr::slice(index_start : index_end) # Keep 1 fold out
      i_df_train <- df_rnd |> dplyr::slice(-(index_start : index_end)) # Keep n-1 fold
      
      # Check if data was not split properly
      # Check if there is at least one date in train that is in validation set
      # If so, promp error
      qc <- any(i_df_train$TIMESTAMP %in% i_df_val$TIMESTAMP)
      if (qc) stop("Error: Data was not split properly!")
      
      # Create recipe based on the ith train set
      i_pp <- 
        recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                        data = i_df_train) |>
        recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
        recipes::step_scale( recipes::all_numeric(), -recipes::all_outcomes())
      
      # Train the model
      i_knn <- 
        caret::train(i_pp, 
                      data = i_df_train |> tidyr::drop_na(), 
                      method = "knn",
                      trControl = caret::trainControl(method = "none"),
                      tuneGrid = data.frame(k = k),
                      metric = "RMSE")
      
      # Extract evaluation on validation set
      i_eval <- 
        eval_model(i_knn, 
                    i_df_train |> tidyr::drop_na(), 
                    i_df_val, 
                    return_metrics = TRUE)
      
      # Create df for metrics
      i_metrics <- 
        tidyr::tibble(
          k    = k,
          fold = i,
          rmse = i_eval$test$.estimate[1],
          rsq  = i_eval$test$.estimate[2],
          mae  = i_eval$test$.estimate[3]
      )
        
      # Append metrics to df
      df_cv <- rbind(df_cv, i_metrics)
    }
    
    # Add some verbose output to know where the loops are at
    cat("\nDone: ", round(match(k, vec_k)/length(vec_k) * 100), " %")
  }
  
  # Return df_cv
  return(df_cv)
}
```


```{r eval=FALSE}
# Function to aggregate cross-validation output to a nice long table
aggregate_cv_metrics <- function(df_cv){
  
  # We must aggregate twice, once for means and once for standard deviations.
  df_agg <- 
    df_cv |> 
    dplyr::group_by(k) |> 
    dplyr::summarise(mean_rmse   = mean(rmse),
              mean_rsq    = mean(rsq),
              mean_mae    = mean(mae),
              sd_rmse    = sd(rmse),
              sd_rsq     = sd(rsq),
              sd_mae     = sd(mae))
  
  df_long <-
    left_join(
      df_agg |> 
        dplyr::select(k, 
                      starts_with("mean_")) |> 
        tidyr::pivot_longer(cols = starts_with("mean_"),
               names_to = "metric",
               values_to = "mean",
               names_prefix = "mean_"),
        df_agg |> 
          dplyr::select(k, 
                        starts_with("sd_")) |> 
          tidyr::pivot_longer(
            cols = starts_with("sd_"),
            names_to = "metric",
            values_to = "sd",
            names_prefix = "sd_"
            )
      )
  
  return(df_long)
}
```

#### Run the cross-validation {.unnumbered}
```{r}
# Pick parameters for cross-validation
my_k <- c(2, 5, 10, 15, 20, 25, 30, 35, 40, 60, 100, 200, 300)
my_folds <- 10
```


<!-- >>> CHECK FOR REDUCING COMPUTATIONAL COST TO RENDER BOOK ONLINE <<< -->
<!-- SAVE DATA WHEN RUNNING LOCALLY BY SETTING EVAL = TRUE, PUSH ONLY WITH EVAL = FALSE -->
```{r eval=FALSE, message=FALSE}
# Run cross validation
df_cv <- cross_validation_by_hand(
  df_train = df_train,
  n_folds  = my_folds,
  vec_k    = my_k
)

# Extract metrics and prepare df for plotting
# Aggregate the metrics plot to the mean values for each metric
df_long <- aggregate_cv_metrics(df_cv)

# Extract value and k for each best metric
df_best <- 
  rbind(
    df_long |> dplyr::filter(metric == "rmse") |> arrange(mean) |> slice(1),
    df_long |> dplyr::filter(metric == "mae") |> arrange(mean) |> slice(1),
    df_long |> dplyr::filter(metric == "rsq") |> arrange(desc(mean)) |> slice(1)
  )
```

```
## Done:  8  %
## Done:  15  %
## Done:  23  %
## Done:  31  %
## Done:  38  %
## Done:  46  %
## Done:  54  %
## Done:  62  %
## Done:  69  %
## Done:  77  %
## Done:  85  %
## Done:  92  %
## Done:  100  %
```

```{r include=FALSE, eval=FALSE}
# Use this only to by-pass running cross-validation when rendering book online
saveRDS(df_best, "data/solutions/smlii_dfbest.rds")
saveRDS(df_long, "data/solutions/smlii_dflong.rds")
```
```{r include=FALSE}
# Load data when rendering book online
df_best <- readRDS("data/solutions/smlii_dfbest.rds")
df_long <- readRDS("data/solutions/smlii_dflong.rds")
```
<!-- >>> END OF CHECK <<< -->

```{r}
# Check what out our dataframes look like
head(df_best)
head(df_long)
```

#### Create output {.unnumbered}

```{r fig.width=6, fig.height=6, fig.align='center'}
# Make a nice facet plot showing how each metric changes with k and 
# with vertical bar showing the optimal k for that parameter
ggplot() +
  geom_line(data = df_long, aes(k, mean)) +
  geom_point(data = df_long, aes(k, mean)) +
  geom_vline(data = df_best,
             aes(xintercept = k),
             color = "red") +    
  facet_wrap(~metric, 
             ncol = 1,
             scales = "free_y",
             labeller = as_labeller(c(
               "rmse" = "RMSE",
               "mae" =  "MAE",
               "rsq" =  "R^2"
             ))) +
  labs(title = "Cross-Validated Hyperparameter Tuning",
       y = "Mean Cross-Validated Metric Value",
       x = "Number of Neighbours (k)") +
  theme_linedraw() +
  theme(plot.title = element_text(face = "bold"))
```


```{r fig.width=6, fig.height=6}
knitr::kable(df_best, 
             digits = 2, 
             caption = "Mean and standard deviation for cross-validated metrics and their best k.")
```

#### Interpretation of results

**What do we see?**
The plot above shows how the cross-validated values for MAE, RMSE, and adj. $R^2$ changes with the number of neighours, $k$. We see that for each metric, we can find a best value at a distinct optimal $k$. Increasing or decreasing $k$ worsens model performance. We can also see that there is some plateauing going on between a $k$ of 15-40, where there are no substantial changes for all metrics.

**What does that imply?**
The pattern of cross-validated metric versus $k$ shows that we should use a value between 15-40 in order to get the best model performance. Alternatively, we could also rerun our analysis with a finer array of $k$ like `seq(20, 40, 1)` to get an even better estimate for $k$.

**How do we continue from here?**
Our goal is to get the best model and report on what is to be expected if we apply this model to unseen data.
Therefore, we now train our model again with the average optimal $k$ found but this time we include all training data. We do this because we want to utilize all available data to create our final model. Then we evaluate this final model against our test data. In theory, the model performance metrics on the test data should fall within the mean and standard deviation from the cross-validated model training. Is that the case here?

#### Teseting our hypothesis {.unnumbered}

<!-- >>> CHECK FOR REDUCING COMPUTATIONAL COST TO RENDER BOOK ONLINE <<< -->
<!-- SAVE DATA WHEN RUNNING LOCALLY BY SETTING EVAL = TRUE, PUSH ONLY WITH EVAL = FALSE -->
```{r eval=FALSE, message=FALSE}
# Get the optimal k from our cross-validation exercise 
my_k <- mean(df_best$k) |> round()

# Run cross validation
df_cv <- cross_validation_by_hand(
  df_train = df_train,
  n_folds  = my_folds, 
  vec_k    = my_k
)

# Aggregate cv-metrics
metrics_final_model <- aggregate_cv_metrics(df_cv)
```
```
## Done:  100  %
```

```{r eval=FALSE}
# Now we train a new model with the same model setup but on the entire training set
# Let's re-use the code from the tutorials!
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = df_train) |> 
  recipes::step_center(recipes::all_numeric(), -recipes::all_outcomes()) |>
  recipes::step_scale( recipes::all_numeric(), -recipes::all_outcomes())

mycv_knn <- 
  caret::train(
    pp, 
    data = df_train |> tidyr::drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "none"),
    tuneGrid = data.frame(k = my_k),
    metric = "RMSE"
)

# Evaluate model against test set
mycv_eval <- eval_model(mycv_knn, 
                        df_train |> tidyr::drop_na(), 
                        df_test, 
                        return_metrics = TRUE)
```


```{r eval=FALSE}
# Let's see what caret::train() estimates
caretcv_knn <- 
  caret::train(
    pp, 
    data = df_train |> tidyr::drop_na(), 
    method = "knn",
    trControl = caret::trainControl(method = "cv", 
                                    number = my_folds),
    tuneGrid = data.frame(k = my_k),
    metric = "RMSE"
)

caretcv_eval <- eval_model(caretcv_knn, 
                           df_train |> tidyr::drop_na(), 
                           df_test, 
                           return_metrics = TRUE)
```

```{r include=FALSE, eval=FALSE}
# Use this only to by-pass running cross-validation when rendering book online
saveRDS(metrics_final_model, "data/solutions/smlii_metrics_final_model.rds")
saveRDS(mycv_knn, "data/solutions/smlii_mycv_knn.rds")
saveRDS(mycv_eval, "data/solutions/smlii_mycv_eval.rds")
saveRDS(caretcv_knn, "data/solutions/smlii_caretcv_knn.rds")
saveRDS(caretcv_eval, "data/solutions/smlii_caretcv_eval.rds")
```
```{r include=FALSE}
# Load data when rendering book online
metrics_final_model <- readRDS("data/solutions/smlii_metrics_final_model.rds")
mycv_knn <- readRDS("data/solutions/smlii_mycv_knn.rds")
mycv_eval <- readRDS("data/solutions/smlii_mycv_eval.rds")
caretcv_knn <- readRDS("data/solutions/smlii_caretcv_knn.rds")
caretcv_eval <- readRDS("data/solutions/smlii_caretcv_eval.rds")
```

<!-- >>> END OF CHECK <<< -->

```{r}
# Finally, compare the cross-validated metrics against the metrics on the test set
cat("\nCross-Validated Metrics by hand: \n",
    " RMSE: ", round(metrics_final_model$mean[1], 3), " +/- ", round(metrics_final_model$sd[1], 3), "\n",
    " R^2: ",  round(metrics_final_model$mean[2], 3), " +/- ", round(metrics_final_model$sd[2], 3), "\n",
    " MAE: ",  round(metrics_final_model$mean[3], 3), " +/- ", round(metrics_final_model$sd[3], 3), "\n",
    
    "\nCross-Validated Metrics by {caret}: \n",
    " RMSE: ", round(caretcv_knn$results[[2]], 3), " +/- ", round(caretcv_knn$results[[5]], 3), "\n",
    " R^2: ",  round(caretcv_knn$results[[3]], 3), " +/- ", round(caretcv_knn$results[[6]], 3), "\n",
    " MAE: ",  round(caretcv_knn$results[[4]], 3), " +/- ", round(caretcv_knn$results[[7]], 3), "\n",
    
    "\nMetrics on Test Set by hand:\n",
    " RMSE: ", round(mycv_eval$test$.estimate[1], 3) ,"\n",
    " R^2: ",  round(mycv_eval$test$.estimate[2], 3) ,"\n",
    " MAE: ",  round(mycv_eval$test$.estimate[3], 3) ,"\n",
    
    "\nMetrics on Test Set by {caret}:\n",
    " RMSE: ", round(caretcv_eval$test$.estimate[1], 3) ,"\n",
    " R^2: ",  round(caretcv_eval$test$.estimate[2], 3) ,"\n",
    " MAE: ",  round(caretcv_eval$test$.estimate[3], 3) ,"\n")
```

#### Interpretation of results {.unnumbered}

Nice, these number look good! They give us two key results:

1.  The cross-validated estimated model metrics are very similar between using our own algorithm and the {caret} package, suggesting that our implementation is correct. Also, the metrics on the test set of the our and {caret}'s model are identical!

2.  When testing our final model, the model that was was trained on the full training set, against the test data, it shows model metrics that are within the range of our estimated cross-validated metrics. For example, the cross-validation shows that if we are training a model on the full dataset and test it on unseen data, we expect that the MAE would fall within 1.148  +/- 0.075. We found an MAE on the test set of 1.149, which is within the expected range. Therefore, we can assume that our model would predict on unseen data with a similar uncertainty.

<!-- Exercise Improvements:
- The splitting between train and validation data does not account for distributions of variables. This could be improved.
-->

### Testing the generalisability of a model {-}

*No solutions provided because part of the final report.*

## Random Forest

### Fitting a Random Forest {-}

Fit a random forest model to the flux data used in the examples of this chapter. Implement bagging 12 decision trees (`num.trees`), each with a maximum depth of 4 (`max.depth`). You can consult the respective arguments for the `"ranger"` method typing `?ranger`. 

```{r}
# Data loading and cleaning
daily_fluxes <- read_csv("./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv") |>
    
  # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all all meteorological covariates
                -contains("JSB")   # weird useless variable
  ) |>
  
  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |>
  
  # set all -9999 to NA
  dplyr::na_if(-9999) |>
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 
  
  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))

# Data splitting
set.seed(123)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# The same model formulation is in the previous chapter
pp <- recipes::recipe(GPP_NT_VUT_REF ~ TA_F + SW_IN_F + LW_IN_F + VPD_F + P_F + WS_F, 
                      data = daily_fluxes_train) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

mod1 <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  tuneGrid = expand.grid( .mtry = floor(6 / 3),        # default p/3
                        .min.node.size = 5,            # default 5
                        .splitrule = "variance"),      # default "variance"
  # arguments specific to "ranger" method
  replace = FALSE,
  max.depth = 4,
  sample.fraction = 0.5,
  num.trees = 12,       
  seed = 1982                                          # for reproducibility
)

# generic print
print(mod1)
```

Repeat the fitting with 1000 decision trees and max depth of 4, then with 12 decision trees and a max depth of 6. Then, discuss the role that the number of decision trees and the maximum depth of a tree play in the bias-variance trade-off and in the computation time. 

```{r}
# Directly fit the model again, with same data and model formulation
# this time num.trees = 1000
mod2 <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  tuneGrid = expand.grid( .mtry = floor(6 / 3),        # default p/3
                        .min.node.size = 5,            # default 5
                        .splitrule = "variance"),      # default "variance"
  # arguments specific to "ranger" method
  replace = FALSE,
  max.depth = 4,
  sample.fraction = 0.5,
  num.trees = 1000,       
  seed = 1982                                          # for reproducibility
)

# generic print
print(mod2)
```

```{r}
# Repeat model fit with max.depth = 6
mod3 <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  tuneGrid = expand.grid( .mtry = floor(6 / 3),        # default p/3
                        .min.node.size = 5,            # default 5
                        .splitrule = "variance"),      # default "variance"
  # arguments specific to "ranger" method
  replace = FALSE,
  max.depth = 6,
  sample.fraction = 0.5,
  num.trees = 12,       
  seed = 1982                                          # for reproducibility
)

# generic print
print(mod3)
```

** Interpretation of results**

```{r}
# check results
rbind(
  mod1$results,
  mod2$results,
  mod3$results
)[,- c(1:3)]
```

Increasing the number of decision trees leads to a decrease in error (e.g. RMSE) and in error variance in the random forest results, making the model more accurate, robust and generalisable. This can be seen by the smaller values of practically all the metrics above. In the bias-variance trade-off, higher `num.trees` shifts the balance towards both lower bias and lower variance.

Increasing the number of variables considered for each split allows for a better fit of the data, but makes the model less generalisable. This is seen by a reduction in the error metrics but an increase in their estimated standard deviation. Hence, using high values for `mtry` may lead to overfitting.

### Hyperparameter tuning {-}

In a previous tutorial, you learned how to tune the hyperparameter $k$ in a KNN by hand. Now you will do the hyperparameter tuning for a Random Forest model. The task gets more complicated because there are more hyperparameters in a random forest. The {caret} package allows to vary three hyperparameters:

-   `mtry`: The number of variables to consider to make decisions at each node, often taken as $p/3$ for regression, where $p$ is the number of predictors.
-   `min.node.size`: The number of data points at the "bottom" of each decision tree, i.e. the leaves.
-   `splitrule`: The function applied to data in each branch of a tree, used for determining the goodness of a decision.

Answer the following questions, giving a reason for your responses:

1.   Check the help for the `ranger()` function and identify which values each of the three hyperparameters/arguments can take. Select a sensible range of values for each hyperparameter, that you will use in the hyperparameter search.

```{r}
mtry_values <- c(2, 4, 6)
min.node.size_values <- c(2, 5, 10, 20)
splitrule_values <- c("variance", "extratrees", "maxstat")
```

2.   In the previous exercise, you have seen how the maximum tree depth regulates fit quality and overfitting. How does the minimum node size relate to tree depth? What happens at the edge cases, when `min.node.size = 1` and when `min.node.size = n` (`n` being the number of observations)? For the next questions, it's not necessary to provide the `max.depth` argument to `train()` because `min.node.size` is already limiting the size of the trees in the random forests.

**Solution**:
The minimum node size is inversely related to the tree depth. The more nodes are required to be at the leaves, the shorter the tree will be, because not so many splits can be done. If we allowed each leave to correspond to a single observation, we would have a very large tree, such that each branch corresponds to one observation. If we force each leave to have at least $n$ observations, then the tree will not "grow", that is, it will never even split.

3.   *Greedy hyperparameter tuning*: Sequentially optimize the choice of each hyperparameter, one at a time and keeping the other two constant. Take the code from the tutorial as a starting point, and those hyperparameter values as an initial point for the search.
> Tip: Keep the number of trees low, otherwise it takes too long to fit each random forest model.

```{r}
# Use data and model formulation created before

results <- c()   # initialise results
set.seed(1997)   # for reproducibility

# Train models in a loop, save metrics
for(mtry_value in mtry_values){
  mod <- caret::train(
    pp, 
    data = daily_fluxes_train %>% 
      drop_na(), 
    method = "ranger",
    metric = "RMSE",
    trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
    tuneGrid = expand.grid( .mtry = mtry_value,           # modify mtry
                          .min.node.size = 5,            # default 5
                          .splitrule = "variance"),      # default "variance"
    # arguments specific to "ranger" method
    replace = FALSE,
    sample.fraction = 0.5,
    num.trees = 100,                                     # keep small for computation
    seed = 1982                                          # for reproducibility
  )
  
  results <- rbind(results, mod$results)
}

results
```

Based on the first round of hyperparameter tuning, we should take `mtry = 2` because it leads to the smallest RMSE and MAE, and the highest Rsquared. That is, the best fit. Nevertheless, the difference between taking `mtry = 2` or `4` is very small and the second actually leads to smaller variance in the metric estimates. Hence, the decision is not so clear.

```{r}
# Take the previous best model and tune next hyperparameter

results <- c()   # initialise results
set.seed(1997)   # for reproducibility

# Train models in a loop, save metrics
for(min.node.size_value in min.node.size_values){
  mod <- caret::train(
    pp, 
    data = daily_fluxes_train %>% 
      drop_na(), 
    method = "ranger",
    metric = "RMSE",
    trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
    tuneGrid = expand.grid( .mtry = 2,      # best mtry
                          .min.node.size = min.node.size_value,   # modify
                          .splitrule = "variance"),      # default "variance"
    # arguments specific to "ranger" method
    replace = FALSE,
    sample.fraction = 0.5,
    num.trees = 100,                                     # keep small for computation
    seed = 1982                                          # for reproducibility
  )
  
  results <- rbind(results, mod$results)
}

results
```

The second hyperparameter should be either `min.node.size = 2` or `5` because they lead to the best metrics overall. Again, the differences are very small and each metric would lead to a different decision. By increasing `min.node.size`, we get more generalisability, but if it's too high we will lose fit quality. If you change the random seed, you'll see that the tuning results are not robust, so whichever hyperparameter we choose won't make a big difference in the model fit (at least within the ranges searched). Let's take `min.node.size = 5` for the next loop.

```{r}
# Take the previous best models and tune last hyperparameter
results <- c()

# Train models in a loop, save metrics
for(splitrule_value in splitrule_values){
  mod <- caret::train(
    pp, 
    data = daily_fluxes_train %>% 
      drop_na(), 
    method = "ranger",
    metric = "RMSE",
    trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
    tuneGrid = expand.grid( .mtry = 4,                    # best mtry
                          .min.node.size = 5,             # best min.node.size
                          .splitrule = splitrule_value),  # modify
    # arguments specific to "ranger" method
    replace = FALSE,
    sample.fraction = 0.5,
    num.trees = 100,                                     # keep small for computation
    seed = 1982                                          # for reproducibility
  )
  
  results <- rbind(results, mod$results)
}

results
```

According to the last round of tuning, we should use `spliturle = "extratrees"`. With that, we found the best model so far.

4.   *Grid hyperparameter tuning*: Starting with the same range of values for each hyperparameter as before, look for the combination that leads to the best model performance among all combinations of hyperparameter values. This time, use the `expand.grid()` function to create a data.frame of hyperparameter value combinations. This grid will be passed to `train()` via the `tuneGrid` argument (see example in the tutorial). This will automatically do the hyperparameter search for you. Comment the output of `train()` and the results of the hyperparameter search.

```{r}
set.seed(1403)    # for reproducibility

mod <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  # expand grid of tunable hyperparameters
  tuneGrid = expand.grid( .mtry = mtry_values,             
                        .min.node.size = min.node.size_values,    
                        .splitrule = splitrule_values),  
  # arguments specific to "ranger" method
  replace = FALSE,
  sample.fraction = 0.5,
  num.trees = 100,                                     # keep small for computation
  seed = 1982                                          # for reproducibility
)

plot(mod, metric = "RMSE")
plot(mod, metric = "Rsquared")
```

5.   Compare the results from the two hyperparameter tuning approaches. Do the optimal hyperparameters coincide? Are the corresponding RMSE estimates similar? What are the advantages and disadvantages of the greedy and the grid approaches?

The best model according to the grid search, with lowest RMSE, is the one with `mtry = 4`, `min.node.size = 5` and `splitrule = "extratrees"` . This is not the "best model" we found with the greedy approach (different `mtry`) and also not the best according to Rsquared (`mtry = 4`, `min.node.size = ` and `splitrule = ""`). The metric used for tuning matters, and looking at several of them at the same can help make decisions, if different metrics agree. All these hyperparameter tuning approaches agree that the best `splitrule` is `"extratrees"` (and if you change the random seed, this result is consistent). The best `mtry` value is different for each split rule used, so having started with `"variance"` in the greedy search lead the tuning in the wrong direction. This highlights that hyperparameter values interact with each other and optimizing over grids is preferred (although it takes more time).

### Model performance {-}

You have trained several random forest models. Evaluate the model performance on the best model (the one for the tuned hyperparameters) and on one of your worse models. If you compare the RMSE and $R^2$ on the training and the test set, does it show overfitting?

```{r}
# Train best model
mod_best <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  # expand grid of tunable hyperparameters
  tuneGrid = expand.grid( .mtry = 6,             
                        .min.node.size = 10,    
                        .splitrule = "extratrees"),  
  # arguments specific to "ranger" method
  replace = FALSE,
  sample.fraction = 0.5,
  num.trees = 100,                                     # keep small for computation
  seed = 1982                                          # for reproducibility
)

# Get predictions

# Train worst model
mod_worst <- caret::train(
  pp, 
  data = daily_fluxes_train %>% 
    drop_na(), 
  method = "ranger",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 5, savePredictions = "final"),
  # expand grid of tunable hyperparameters
  tuneGrid = expand.grid( .mtry = 2,             
                        .min.node.size = 20,    
                        .splitrule = "maxstat"),  
  # arguments specific to "ranger" method
  replace = FALSE,
  sample.fraction = 0.5,
  num.trees = 100,                                     # keep small for computation
  seed = 1982                                          # for reproducibility
)

source("R/eval_model.R")
eval_model(mod_best, daily_fluxes_train, daily_fluxes_test)
eval_model(mod_worst, daily_fluxes_train, daily_fluxes_test)
```

The performance on the test set for the best model is still close to the performance on the training set, so the model doesn't seem to overfit. The same goes for the worse model, which leads to worse $R^2$ and RMSE and visually the fit is slightly worse.
