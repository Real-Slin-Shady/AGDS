# Data variety {#data_variety}

**Chapter lead author: Koen Hufkens**

Contents:

-   Lecture (Mirko): Mapping data
-   Data formats, standards, metadata
-   Geographic data
-   Scraping, wget
-   APIs

## Introduction

As a scientist you will encounter variety of [data]([https://en.wikipedia.org/wiki/Data\_(computing)](https://en.wikipedia.org/wiki/Data_(computing))) (formats). In this section you will learn some of the most common formats around, their structure, and the advantages and disadvantages of using a particular data formats. Only singular files are considered in this section, and [databases](<https://en.wikipedia.org/wiki/Database>) are not covered although some files (formats) might have a mixed use.

However, more and more data moves toward a cloud server based model where data is queried from an online database using an [Application Programming Interface (API)](<https://en.wikipedia.org/wiki/API>). Although the explicit use of databases is not covered you will learn basic API usage, to query data which is not represented as a file.

In this section you will learn:

-   how to recognize data/file formats

-   understand data/file format limitations

    -   manipulation wise

    -   content wise

-   how to read and or write data in a particular file format

-   how to query an API

    -   and store it locally

## Files and file formats

### File extensions

In order to manipulate data and make some distinctions on what a data file might contain they carry a particular [file format extension](<https://en.wikipedia.org/wiki/Filename_extension>). This file extensions denotes the intended content and use of a particular file.

For example a file ending in `.txt` suggests that it contains text. A file extension allows you, or a computer, to assess the content of a file without opening the file.

File extensions are therefore an important tool in assessing what data you are dealing with, and what tools you will need to manipulated (read / write) the data.

> NOTE: It is important to note that file extensions can be changed. In some cases the file extension does not represent the content and or use case of the data contained within the file.

> TIP: If a file doesn't read it is always wise to check the first couple of lines to verify if the data has a structure which corresponds to the file extension.
>
> ``` bash
> # On a linux/macos system you can use the below command
> # to show the first couple of lines of a file
> head your_file
>
> # alternatively you can show the last few lines
> # of a file using
> tail your_file
> ```

#### Human readable data

One of the most important distinctions in data formats falls along the line of it being human readable or not. Human readable data is, as the term specifies, made up of normal text characters. Human readable text has the advantage that it is easily read, and or edited using conventional text processors. This convenience comes at the cost of the files not being compressed in any way, and file sizes can become unsustainable. However, for many applications where file sizes are limited (\<50MB), human readable formats are the preferred option. Most human readable data falls in two broad categories, tabulated data and structured data.

##### Tabulated data

Often, human readable formats provide data in tabulated form using a consistent delimiter. This delimiter is a character separating columns of a table.

    column_one, column_two, column_three
    1, 2, 3
    1, 2, 3
    1, 2, 3

Common delimiters in this context are the comma (,), as shown in the above example. A file with this particular format often carries the [comma-separated values file extension (*.csv)](<https://en.wikipedia.org/wiki/Comma-separated_values>). Other delimiters are the [tabulation (tab) character](<https://en.wikipedia.org/wiki/Tab_key#Tab_characters>). Files with tab delimited values have the* format.

> TIP: File extensions aren't always a true indication of the delimiter used. For example, `.txt` files often contain comma or tab separated data. If reading a file using a particular delimiter fails it is best to check the first few lines of a file.

##### Structured data

Tabulated delimited data is row and column oriented and therefore doesn't allow complex structured content, e.g. tables within tables. This issue is sidestepped by for example the [JSON format](<https://www.json.org/json-en.html>). The JSON format in particular uses attribute-value pairs to store data, and is therefore more flexible in terms of accommodating varying data structures. Below you see an example of details describing a person, with entries being fo varying length and data content.

``` json
{
  "firstName": "John",
  "lastName": "Smith",
  "isAlive": true,
  "age": 27,
  "address": {
    "streetAddress": "21 2nd Street",
    "city": "New York",
    "state": "NY",
    "postalCode": "10021-3100"
  },
  "phoneNumbers": [
    {
      "type": "home",
      "number": "212 555-1234"
    },
    {
      "type": "office",
      "number": "646 555-4567"
    }
  ],
  "children": [
      "Catherine",
      "Thomas",
      "Trevor"
  ],
  "spouse": null
}
```

Note, despite being human readable, a JSON file is considerably harder to read than a comma separated file. Editing such a file is therefore more prone to errors if not automated.

Other human readable structured data formats include the eXtensible Markup Language (XML), which is commonly used in web infrastructure. XML is used for storing, transmitting, and reconstructing arbitrary data but uses (text) markup instead of attribute-value pairs.

``` xml
<note>
<to>Tove</to>
<from>Jani</from>
<heading>Reminder</heading>
<body>Don't forget me this weekend!</body>
</note>
```

#### Binary data

All digital data which is not represented as text characters can be considered binary data. Binary data can vary in its content from the executable, which runs a program, to the digital representation of an image. However, in all cases the data is represented as bytes (made of eight bits) and not text characters.

One of the advantages of binary data is that it is an efficient representation of data, saving spaces. This comes at the cost of requiring a dedicated software, other than a text editor, to manipulate the data. For example, digital images in a binary format require image manipulation software.

More so than human readable data the file format (extension) determines how to treat the data. Knowing common data formats and their use cases is therefore key.

### File formats

Environmental sciences have particular file formats which dominate the field. Some of these file formats relate to the content of the data, some of these formats are legacy formats due to the history of the field itself. Here we will list some of the most common formats you will encounter.

| File format (extension) | Format description                                               | Use case                                                                                                                            | R Library     |
|------------------------|----------------|----------------|----------------|
| \*.csv                  | comma separated tabulated data                                   | General purpose flat files with row and column oriented data                                                                        | base R        |
| \*.txt                  | tabulated data with various delimiters                           | General purpose flat files with row and column oriented data                                                                        | base R        |
| \*.json                 | structured human readable data                                   | General purpose data format. Often used in web application. Has geospatial extensions (geojson).                                    | jsonlite      |
| \*.nc                   | [NetCDF data](<https://en.wikipedia.org/wiki/NetCDF>) array data | Array-oriented data (matrices with \> 2 dimensions). Commonly used to store climate data or model outputs. Alternative to HDF data. | ncdf4         |
| \*.hdf                  | HDF array data                                                   | Array-oriented data (matrices with \> 2 dimensions). Commonly used to store climate data or model outputs.                          | hdf           |
| \*.tiff, \*.geotiff     | Geotiff multi-dimensional raster data (see below)                | Layered (3D) raster (image) data. Commonly used to represent spatial (raster) data.                                                 | terra, raster |
| \*.shp                  | Shapefile of vector data (see below)                             | Common vector based geospatial data. Used to describe data which can be captured by location/shape and attribute values.            | sf            |

## Meta-data

Meta-data, or data which is associated with the main data file is key to understanding the content of a data file (or the data set to which the file belongs). In some cases you will find this data only as a general description referencing the file(s) itself. In other cases meta-data is included in the file itself.

For example, many tabular CSV data files contain a header specifying the content of each column, and at times a couple of lines of data specifying the content of the file itself - or context within which the data should be considered.

``` bash
# This is meta-data associated with the tabulated CSV file
# for which the data is listed below.
# 
# In addition to some meta-data, the first row of the data
# contains the column header data
column_one, column_two, column_three
1, 2, 3
1, 2, 3
1, 2, 3
```

In the case of binary files it will not be possible to read the meta-data as plain text. In this case, specific commands can be used to read the meta-data included in a file. The example below shows how you would list the meta-data of a GeoTiff file.

``` bash
# list geospatial data for a geotiff file
gdalinfo your_geotiff.tiff
```

> TIP: Always keep track of your meta-data if not, or only partially, included in the file itself. Meta-data is key in making science reproducible and guaranteeing consistency between projects. Key meta-data to retain are: the source of your data (url, manuscript, doi), the date when the data was downloaded, and any manipulations on the data before using the data in a final workflow.

## Spatial data representation

Environmental data often has an explicit spatial and temporal component. For example, climate data is often represented as 2D maps which vary over time. This spatial data requires an additional level of understanding of commonly used data formats and structures.

In general, we can distinguish two important data models when dealing with spatial data, the [raster](<https://wiki.gis.com/wiki/index.php/Raster_data_model>) and [vector data model](<https://wiki.gis.com/wiki/index.php/Vector_data_model>). Both data have their typical file formats (see above) and particular use cases. The definition of these formats, optimization of storage and math/logic on such data are the topic of Geographic Information System (GIS) science and beyond the scope of this course. We refer to other elective GIS courses for a greater understanding of these details. However, a basic understanding of both raster and vector data is provided.

### Raster data model

The basic raster model represents geographic (2D) continuous data as a two-dimensional array, where each position has a geographic (x, y) coordinate, a cell size (or resolution) and a given extent. Using this definition any image adheres to the raster model. However, in most geographic applications coordinates are referenced and correspond to a geographic position, e.g. a particular latitude longitude. Often, the model is expanded with a time dimension, stacking various two-dimensional arrays into a three-dimensional array.

The raster data model is common for all data sources which use either imaging sensors, such as satellites or UAVs, or model based output which operates on a fixed grid, such as climate/weather models.

Additional meta-data stores both the geographic reference system, the time components as well as other data which might be helpful to end users. Within the environmental sciences NetCDF and GeoTiff are common raster data file formats.

![](<https://eo4geo.sbg.ac.at/GEOF/Basic-GIS-knowledge-and-raster-data/rastervector.png>)

### Vector data model

The vector data model, in contrast to the raster data model, describes (unbound) features using a geometry (location, shape) using coordinates and linked feature attributes. Geometries can be points, lines, polygons, or even volumes.

Vector data does not have a defined resolution, making them scale independent. This makes the vector data model ideal for discrete features such as road or building outlines. Conversely, vector data is poorly suited for continuous data.

Conversions between the vector and raster model are possible, but limitations apply. For example, when converting vector data to raster data a resolution needs to be specified, as you lose scale independence of the vector format. Conversions from raster to vector are similarly limited by the original resolution of the raster data.

## Data sources 

The sections above assume that you have inherited some data from someone, or have data files on disk (in a particular format). Yet, most of the time gathering data is the first step in any analysis. Depending on where data is hosted you can simply download data through your web browser and get on with it.

But, many of the data described in previous sections are currently warehoused in large cloud facilities. These data (and their underlying data formats) are stored in large databases and displayed through various applications. For example, Google Maps displays remote sensing (satellite) raster image data in addition to street level vector based labels. These services allow you to access the underlying (original) data using an API, hence programmatically using code. Mastering the use of these services is key in gathering research data.

### Direct downloads

Before diving into a description of APIs we remind you that some file reading functions in R are web aware, and not only can read local files but also remote ones (i.e., URLs). Getting ahead of ourselves a bit (see tutorials below), the example code shows you how to read the content of a URL directly into your R environment.

Although using this functionality isn't equivalent to using an API, the concept is the same i.e., you load a remote data source.

``` r
# define a URL with data of interest
# in this case annual mean CO2 levels at Mauna Loa
url <- "https://gml.noaa.gov/webdata/ccgg/trends/co2/co2_annmean_mlo.csv"

# read in the data directly from URL
df <- read.table(
  url,
  header = TRUE,
  sep = ","
)
```

### APIs

Web based Application Programming Interfaces (APIs) offers a way to specify the scope of the returned data, and ultimately the processing which goes on behind the scene in response to a (data) query. APIs are a way to, in a limited way, control a remote server to execute a certain (data) action. In most (RESTful) APIs such query takes the form of an HTTP URL via an URL encoded scheme using an API endpoint (or base URL).

#### GET

Depending on your data source you will either need to rely on a dedicated R package to query the API or study the API documentation. A dedicated package to query an API is for example the {ecmwfr} package to download ECMWF climate data from the Copernicus data services.

The general scheme to follow to use an API follows the use of the GET() command of the {httr} R library. You define a query using API parameters, as a named list, and then use a GET() statement to download the data from the endpoint (url).

``` r
  # formulate a named list query to pass to httr
  query <- list(
    "argument" = "2",
    "another_argument" = "3"
  )
  
  # create url string (varies per product / param)
  url <- "https://your.service.endpoint.com"
  
  # download data using the
  # API endpoint and query data
  status <- httr::GET(
    url = url,
    query = query,
    httr::write_disk(
      path = "/where/to/store/data/filename.ext",
      overwrite = TRUE
      )
    )
```

##### Authentication

Depending on the API authentication using a user name and a key or password is required. Then the template should be slightly altered to accommodate for these requirements. Note that instead of the GET() command we use POST() as we need to post some authentication data before we can get the data in return.

``` r
# an authenticated API query
status <- httr::POST(
  url = url,
  httr::authenticate(user, key),
  httr::add_headers("Accept" = "application/json",
                    "Content-Type" = "application/json"),
  body = query,
  encode = "json"
)
```

## Learning objectives

## Tutorial

## Exercises

## Solutions
