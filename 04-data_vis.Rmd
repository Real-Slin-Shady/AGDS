# Data visualisation {#data_vis}

**Chapter lead author: Benjamin Stocker**

Contents:

-   Lecture (Isabelle Bentz?): The art of visualising data, grammar of graphics
-   Exercise: Develop decision tree for what type of visualisation to apply
-   Performance assessment: Interactive work sequence

## Learning objectives

## Required packages

```{r message=FALSE, warning=FALSE}
use_pkgs <- c("dplyr", "tidyr", "readr", "ggplot2", "cowplot", "lubridate")
new_pkgs <- use_pkgs[!(use_pkgs %in% installed.packages()[, "Package"])]
if (length(new_pkgs) > 0) install.packages(new_pkgs)
invisible(lapply(use_pkgs, require, character.only = TRUE))
```

## Tutorial

Visualizations often take the center stage of publications and are often the main vehicles for transporting information in scientific publications and (ever more often) in the media. Visualizations communicate data and its patterns in visual form. Visualizing data is also an integral part of the exploratory data analysis cycle. Visually understanding the data guides its transformation and the identification of suitable models and analysis methods.

The quality of a data visualization can be measured by its effectiveness of conveying information about the data and thus of answering a question with the data and telling a story. Different aspects determine this effectiveness, including the appropriateness of visualization elements, the intuitiveness of how information can be decoded from the visualization by the reader, the visual clarity and legibility (taking into account the vision and potential vision deficiencies of the reader), the visual appeal, etc. This tutorial introduces data visualization under the premise that not all aspects of data visualization are a matter of taste. There are appropriate and less appropriate ways of encoding data in visual form.

This tutorial is inspired by the comprehensive and online available textbook Fundamentals of [Data Visualization by Claus O. Wilke](https://clauswilke.com/dataviz/index.html).

### The grammar of graphics

In Chapter \@ref(data_wrangling), we learned about axes of variation in the data. For example, time is an axis of variation in our example data `hhdf`, or site identity and the date are axes of variation in our example data `ddf`. We have also learned that we can aggregate over axes of variation, and that we can often separate an axis of variation into a hierarchy of subordinate axes of variation (e.g., years, months, days, and a half-hourly time axis).

In this chapter, we will be working mainly with the same half-hourly time series data of ecosystem-atmosphere fluxes and parallel measurements of meteorological variables - as in Chapters \@ref(getting_started) and \@ref(data_wrangling). For time series data, the entry point of the exploratory data analysis cycle may be a visualization of some variable of interest (here `GPP_NT_VUT_REF`) against time:

```{r}
hhdf <- read_csv("data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv")
plot(hhdf$TIMESTAMP_START, hhdf$GPP_NT_VUT_REF, type = "l")
```
You may notice the spurious-looking values on the left, in the first third of year 2004. We'll revisit this point later in this Chapter.

From [(Wilke)](https://clauswilke.com/dataviz/):

"All data visualizations map data values into quantifiable features of the resulting graphic. We refer to these features as *aesthetics*."

Applied to our example, the aesthetics are the x-axis and the y-axis of a cartesian coordinate system. `TIMESTAMP_START` is mapped onto the x-axis, `GPP_NT_VUT_REF` is mapped onto the y-axis, and their respective values specify the position of points in the cartesian coordinate system that are then connected with lines - making up the geometrical object that represents the data. Often, the aesthetic that is used to plot the target variable against corresponds to a known axis of variation in the data.

The notion of mapping data onto aesthetics and using objects whose geometry is defined by the aesthetics gives rise to the *grammar of graphics* and to the *ggplot2* R package for data visualisation (which we will use throughout the remainder of this course). The equivalent *ggplot2* code that follows the philosophy of the grammar of graphics is:

```{r}
ggplot(data = hhdf, aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) +
  geom_line()
```

The argument provided by the `aes()` statement specifies the aesthetics (`x`, and `y`) and which variables in `data` are mapped onto them. Once this is specified, we can use any suitable geometrical object that is defined by these aesthetics. Here, we used a line plot specified by `+ geom_line()`.

The data visualisation above is a dense plot and we cannot distinguish patterns because variations in GPP happen at time scales that are too narrow for displaying three years of half-hourly data in one plot. GPP varies throughout a day just as much as it varies throughout a season. To see this, we can focus on a narrower time span (selecting rows by index using `slice()` in the code below). Visual clarity is also facilitated by an appropriate labeling (title and axes labels using `labs()`) and by a reduction of displayed elements to a minimum (therefore, the changing of the formatting theme by `theme_classic()`):

```{r}
hhdf |> 
  slice(24000:25000) |>
  ggplot(aes(x = TIMESTAMP_START, y = GPP_NT_VUT_REF)) +
  geom_line() +
  labs(title = "Gross primary productivity", 
       subtitle = "Site: CH-Lae",
       x = "Time", 
       y = expression(paste("GPP (gC m"^-2, "s"^-1, ")"))) +
  theme_classic()
```

Find a complete reference to *ggplot2* [here](https://ggplot2.tidyverse.org/) [@R-ggplot2].

### Every data has its representation

In the above example, we mapped two continuous variables (`TIMESTAMP_START` and `GPP_NT_VUT_REF`) onto the aesthetics `x`, and `y` to visualize time series data. A line plot is an appropriate choice for such data as points are ordered along the time axis and can be connected by a line. Different "geometries" are suitable for visualizing different aspects of the data, and different variable types are suited to mapping onto different aesthetics. Common, available aesthetics are shown in Fig. XXX and can be allocated to variable types:

-   Continuous variables: position, size, color (a color gradient) , line width
-   Categorical variables: shape, color (a discrete color set), line type

<!-- https://clauswilke.com/dataviz/aesthetic_mapping_files/figure-html/common-aesthetics-1.png -->

![](./fig/common-aesthetics-1.png)

Not only the different aesthetics, but also the each type of *geometry* (the layers of the visualization added to a plot by `+ geom_*()`) goes with certain types of variables and aspects of the data (but not with others). The sub-sections below provide a brief overview into the categorization of data visualization types. A more comprehensive overview is given by [GGPlot2 Essentials for Great Data Visualization in R by Alboukadel Kassambara](http://www.sthda.com/english/wiki/be-awesome-in-ggplot2-a-practical-guide-to-be-highly-effective-r-software-and-data-visualization#geom_bar-bar-plot).

#### One value per category

Probably the simplest case of data vizualisation is where a single value is shown across a categorical variable. This calls for a bar plot (`geom_bar()`). In the example below, we plot the mean GPP for within each month. The "custom plot" shown below is a demonstration for what you can do by combining different elements with *ggplot2*. Try to understand the command for creating the object `gg2`.

```{r}
gg1 <- ddf |> 
  mutate(month = month(date, label = TRUE)) |> 
  group_by(month) |> 
  summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |> 
  ggplot(aes(x = month, y = GPP_NT_VUT_REF)) +
  geom_bar(stat = "identity") +
  theme_classic() +
  labs(title = "Bar plot",
       x = "Month", 
       y = expression(paste("GPP (gC m"^-2, "s"^-1, ")")))

gg2 <- ddf |> 
  mutate(month = month(date, label = TRUE)) |> 
  group_by(month) |> 
  summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |> 
  ggplot(aes(x = month, y = GPP_NT_VUT_REF)) +
  geom_segment(aes(x = month, xend = month, y = 0, yend = GPP_NT_VUT_REF), 
               size = 3, color = "grey40") +
  geom_point(aes(x = month, y = GPP_NT_VUT_REF), size = 8, color = "grey40") +
  geom_text(aes(x = month, y = GPP_NT_VUT_REF, label = format(GPP_NT_VUT_REF, digits = 2)),
            size = 3, color = "white") +
  theme_classic() +
  labs(title = "Custom plot",
       x = "Month", 
       y = expression(paste("GPP (gC m"^-2, "s"^-1, ")"))) +
  scale_y_continuous(limits = c(0, 8.75), expand = c(0, 0)) +
  coord_flip()
  
plot_grid(gg1, gg2)
```

Above, we created two objects, `gg1` and `gg2`, that contain the instructions for creating the plots. To combine multiple sub-plots within panels of a single plot, we used `plot_grid()` from the *cowplot* library.

Note also the `stat = "identity"` specification within the `geom_bar()` function call. This is required when the bar height is specified by a single value within each category (`month` in the example above). To visualize not a value *per se* but the count of values within categories, use `stat = "count"` to get the equivalent result as when aggregating by taking the number of observations within categories explicitly using the *dplyr* function `summarise()`. This equivalency is demonstrated below.

```{r}
gg1 <- hhdf |> 
  filter(NEE_VUT_REF_QC == 0) |> 
  group_by(NIGHT) |> 
  summarise(count = n()) |> 
  ggplot(aes(x = NIGHT, y = count)) +
  geom_bar(stat = "identity") +
  theme_classic()

gg2 <- hhdf |> 
  filter(NEE_VUT_REF_QC == 0) |> 
  ggplot(aes(x = NIGHT)) +
  geom_bar(stat = "count") +
  theme_classic()

plot_grid(gg1, gg2)
```

#### Distribution of one variable

Examining the distribution of a variable is often the first step of exploratory data analysis. A *histogram* displays the distribution of numerical data by mapping the frequency (or count) of values within discrete bins (equally spaced ranges along the full range values the respective variable attains) onto the "height" of a bar, and the range of values itself onto the position of the bar. In other words, it shows the count of how many points of a certain variable (below `GPP_NT_VUT_REF`) fall into a discrete set of bins. When normalizing (scaling) the "bars" of the histogram to unity, we get a *density histogram*. To specify the y-axis position of the upper end of the histogram bar as the density, use `y = ..density..` in the `aes()` call. To show counts, use `y = ..count..`.

```{r}
hhdf |>
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..density..)) +
  geom_histogram(fill = "grey70", color = "black") +
  geom_density(color = "red") +  # we can overlay multiple plot layers!
  labs(title = "Histogram and density", x = expression(paste("GPP (gC m"^-2, "s"^-1, ")"))) +
  theme_classic()
```

Note that the red line plotted by `geom_density()` on top of the density histogram visualises the *density distribution* in continuous (not discrete, or binned) form. Note also that both "geoms" share the same aesthetics with `aes()` specified in the `ggplot()` function call.

#### Distributions within categories

To visualise distributions of a single continuous variable within categories, perhaps the most common visualisation type is the *box plot*. As described in Chapter \@ref(data_wrangling), it shows the median (bold line in the center), the upper and lower quartiles, corresponding to the 25% and the 75% quantiles, often referred to as $Q_1$ and $Q_3$ , and given by the upper and lower edge of the box plot. The lines extending from the box edges visualize the range of $( Q_1 - 1.5 (Q_3 - Q_1)$ to $Q_3 + 1.5 (Q_3 - Q_1)$. Any point outside this range is plotted by a point.

The box plot is rather reductionist in showing the data (the vector of all values is reduced to the median, \$Q_1 , $Q_3$, and outlying points) and may yield a distorted picture of the data distribution. For this reason, several journals are now requiring individual data points or at least the number of data points to be shown in addition to the boxes. Below, points are added by `geom_jitter()` , where points are "jittered", that is, randomly spread out along the x-axis. Violin plots are a hybrid of a density plot and a box plot. The form of their edge is given by the density distribution of the points they represent.

```{r}
set.seed(1985)
gg1 <- hhdf |> 
  sample_n(300) |> 
  mutate(Night = ifelse(NIGHT == 1, TRUE, FALSE)) |> 
  ggplot(aes(x = Night, y = VPD_F)) +
  geom_boxplot(fill = "grey70") +
  labs(title = "Box plot") +
  labs(y = "VPD (hPa)") +
  theme_classic()

set.seed(1985)
gg2 <- hhdf |> 
  sample_n(300) |> 
  mutate(Night = ifelse(NIGHT == 1, TRUE, FALSE)) |> 
  ggplot(aes(x = Night, y = VPD_F)) +
  geom_boxplot(fill = "grey70", outlier.shape = NA) +
  geom_jitter(width = 0.2, alpha = 0.3) +
  labs(title = "Boxplot + jittered points") +
  labs(y = "VPD (hPa)") +
  theme_classic()

set.seed(1985)
gg3 <- hhdf |> 
  sample_n(300) |> 
  mutate(Night = ifelse(NIGHT == 1, TRUE, FALSE)) |> 
  ggplot(aes(x = Night, y = VPD_F)) +
  geom_violin(fill = "grey70") +
  labs(title = "Violin plot") +
  labs(y = "VPD (hPa)") +
  theme_classic()

plot_grid(gg1, gg2, gg3, ncol = 3)
```

#### Regression of two continuous variables

Scatter plots visualize how two variables co-vary. The position of each point in a scatter plot is given by the simultaneously recorded value of two variables, provided in two columns along the same row in a data frame, and mapped onto two dimensions in a cartesian coordinate system. We can also say that two variables are *regressed* against each other.

In the figure below, we start with a simple scatter plot (a), regressing GPP against shortwave radiation. A visualization is supposed to tell a story with data. The positive and largely linear relationship between shortwave radiation and GPP is expected from theory (Monteith, 1972) and our process understanding of the dominant controls on photosynthesis - it's mainly solar (shortwave) radiation. The linear regression line, added by `geom_smooth(method = "lm")` in (a), indicates that relationship.

```{r}
gg1 <- hhdf |>
  sample_n(1000) |>  # to reduce the dataset
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +
  geom_point(size = 0.75) +
  geom_smooth(method = "lm", color = "red") +
  labs(x = expression(paste("Shortwave radiation (W m"^-2, ")")), 
       y = expression(paste("GPP (gC m"^-2, "s"^-1, ")"))) +
  theme_classic()

gg2 <- hhdf |>
  sample_n(1000) |> 
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = NIGHT)) +
  geom_point(size = 0.75) +
  labs(x = expression(paste("Shortwave radiation (W m"^-2, ")")), 
       y = expression(paste("GPP (gC m"^-2, "s"^-1, ")"))) +
  theme_classic()

gg3 <- hhdf |>
  sample_n(1000) |> 
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = as.factor(NIGHT))) +
  geom_point(size = 0.75) +
  labs(x = expression(paste("Shortwave radiation (W m"^-2, ")")), 
       y = expression(paste("GPP (gC m"^-2, "s"^-1, ")"))) +
  theme_classic()

gg4 <- hhdf |>
  sample_n(1000) |> 
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = TA_F)) +
  geom_point(size = 0.75) +
  labs(x = expression(paste("Shortwave radiation (W m"^-2, ")")),
       y = expression(paste("GPP (gC m"^-2, "s"^-1, ")"))) +
  theme_classic() +
  scale_color_viridis_c()

plot_grid(gg1, gg2, gg3, gg4, ncol = 2, labels = "auto")
```

Are there additional variables that modify the relationship between solar radiation and GPP? To illustrate this, we can map additional variables in our data set onto additional aesthetics. For example, at night, photosynthesis ceases (b). Here, the variable `NIGHT` was mapped onto the aesthetic `color` of same geometry (`geom_point()`). By default, `ggplot()` used a *continuous color scale*, as indicated by the color key on the right. It did so although `NIGHT` is a categorical (a binary) variable because in the data frame, `NIGHT` is stored as a numeric value (as can be checked by `class(hhdf$NIGHT)`). To avoid this, and automatically trigger the use of a color scheme that is suitable for categorical variables, we specify `aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = as.factor(NIGHT))` in (d).

The above example demonstrates that color schemes have to be chosen depending on the nature of the data. Mapping a continuous variable onto the aesthetics `color` requires a continuous color scheme to be applied, categorical data requires discrete color schemes. Two more distinctions have to be considered:

- Continous variables should be distinguished further if they span a range that includes zero or not. If so, *diverging* color schemes should be used, where zero appears neutral (e.g., white). If zero is not contained within the range of values in the data, diverging color schemes should be avoided.
- Continuous or ordinal variables may be cyclic in nature. For example, hours in a day are cyclic, although there are twelve discrete numbers. Note that 00:00 is nearer to 23:59 than it is from, for example, 01:00. The cyclical, or periodical nature of the data should be reflected in the choice of a color scheme where the edges of the range are more similar to one another than they are to the center of the range.

Colors in color schemes (or scales) should be:

- Distinguishable for people with color vision deficiency
- Distinguishable when printed in black and white
- Evenly spaced in the color space
- Intuitively encoding the information in the data (for example, blue-red for cold-hot)
- Visually appealing

In (d) of the figure above, we mapped temperature, a continuous variable, onto the color aesthetic of the points and chose the continuous [*viridis*](https://ggplot2.tidyverse.org/reference/scale_viridis.html) color scale by `+ specifying scale_color_viridis_c()`. The viridis scales have become popular for their respect of the points listed above.

For further reading, several excellent resources exist that theoretize and guide the use of color in data visualisation. Excellent sources are:

- [Fabio Crameri's Scientific colour maps](https://www.fabiocrameri.ch/colourmaps/), [Crameri (2018)](https://gmd.copernicus.org/articles/11/2541/2018/) and its R package *scico* (on CRAN).
- [Paul Tol's Notes](https://personal.sron.nl/~pault/), available for example in the *khroma* R package (on CRAN).

#### Regression within categories

In the sub-plot (d) above, we may observe a pattern: GPP recorded at low temperatures (dark colored points) tend to be located in the lower range of the cloud of points. We may formulate a hypothesis from this observation, guiding further data analysis and modelling. This illustrates how data visualisation is an integral part of any (geo-) data science workflow.

Since the relationship between incoming solar radiation and ecosystem photosynthesis is strongly affected by how much of this light is actually absorbed by leaves, and because the amount of green foliage varies strongly throughout a year (the site CH-Lae is located in a mixed forest), the slope of the regression between solar radiation and GPP should change between months.

Hence, let's consider months as the categories to be used for separating the data and analysing the bi-variate relationships separately within. Below, two alternatives are presented. Either the data is separated into a grid of sub-plots, or the data is separated by colors within the same plot panel.

**Separation by color**

```{r}
ddf |>
  mutate(month = month(date, label = TRUE)) |>
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = month)) +
  geom_point(alpha = 0.5) +
  geom_smooth(formula = y ~ x + 0, method = "lm", se = FALSE) +
  labs(x = expression(paste("PPFD (", mu, "mol m"^-2, "s"^-1, ")")), 
       y = expression(paste("GPP (gC m"^-2, "s"^-1, ")")) ) +
  theme_classic()
  # khroma::scale_colour_berlin()
```

Note that here, the color-mapping is specified within `aes()` in the `ggplot()` function call and then adopted for all subsequent additions of geoms. Hence, also the `geom_smooth()` thus takes the color information, and not by a "hard-coded" specification of `color =` inside the `geom_smooth()` call as done in Fig. XXX.

Note also that we specified a formula for the linear regression "smooting curve" to force the lines through the origin (`y ~ x + 0`).

**Separation into sub-plots**

Yet another "mapping" is available with `facet_wrap()`. It separates the visualisation into different sub-plots, each showing only the part of the data that falls into the respective category, separated by `facet_wrap()`. Note, this mapping is not dealt with the same way as other aesthetics - not with specifying it with `aes()`), but with adding the `facet_wrap()` with a `+` to the `ggplot()` object. The variable by which `facet_wrap()` separates the plot has to be specified as an argument with a preceeding `~`. Here, this is `~month`.

```{r}
ddf |>
  mutate(month = month(date, label = TRUE)) |>
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +
  geom_point(alpha = 0.4) +
  geom_smooth(formula = y ~ x + 0, method = "lm", color = "red", se = FALSE) +
  labs(x = expression(paste("PPFD (", mu, "mol m"^-2, "s"^-1, ")")), 
       y = expression(paste("GPP (gC m"^-2, "s"^-1, ")")) ) +
  facet_wrap(~month)
```

You may object here that a linear regression is not a good model for our data. Instead, the relationship looks saturating, as indicated for example by the data in August. But we'll get to modelling in later chapters.

The two visualisations above confirm our suspicion that the light-GPP relationship varies between months.

#### Time series

A time series plot can be regarded as a special case of a regression of two variables. In this case, one variable is regressed against time. Although time is continuous *per se*, its representation in a data set is (necessarily) discrete and ordered. There is a natural order in time steps. Therefore, it makes sense to visualise data data using lines that connect the points. The example below shows the time series of daily GPP in year 2005.

```{r}
ddf |> 
  #filter(year(date) %in% 2005:2006) |>  # same functions as above can be applied to 'date'
  ggplot(aes(x = date, y = GPP_NT_VUT_REF)) +
  geom_line() +
  geom_smooth(se = FALSE, color = "red") +
  labs(title = "Line plot", x = "Time", y = expression(paste("GPP (gC m"^-2, "s"^-1, ")"))) +
  theme_classic()
```

After completing the aggregation above, we now have a new "hidden dimension" in our data frame: Each GPP measurement is located not only along a time axis, but also along a "data quality axis", measured by the fraction of actually measured (not gap-filled) half-hourly data points per day (`f_measured`). In the next part of the tutorial, we will show you how to make the most of such "hidden dimensions" by adding them as an additional layer to our data plots using extra colours or regressions.

We will proceed by using this additional axis to enhance our plot and visualising it the extra dimension in the data. We achieve this by colouring our points according to `f_measured`. In other words, we "map" `f_measured` to the color axis, similar to how we "mapped" time and GPP to the x and y axes before. When adding such an additional mapping to visualisation dimensions ("aesthetics"), we have to specify it using `aes()`. This only affects the points and the color of points, while the lines and points and their position in x-y space is shared. Hence, we write `aes(x = date, y = GPP_NT_VUT_REF)` in the `ggplot()` function call (indicating that all subsequent additions of `geom_` layers share this x-y mapping); while `aes(color = f_measured)` is specified only in the `geom_point()` layer.

```{r}
ddf |> 
  ggplot(aes(x = date, y = GPP_NT_VUT_REF)) +
  geom_line() +
  geom_point(aes(color = f_measured), size = 0.9) +  # we can overlay multiple plot layers!
  labs(title = "Gross primary productivity", 
       subtitle = "Site: CH-Lae",
       x = "Time", y = expression(paste("GPP (gC m"^-2, "s"^-1, ")"))) +
  scale_color_viridis_c(direction = -1) + # "viridis" continuous color scale in inverse direction
  theme_classic()
```

We observe that the points with particularly low GPP during summer months are predominantly based on gap-filled half-hourly data. This is an insight we would never have gotten by just looking at the naked values in our data frames. Data visualisations are essential for guiding analyses and processing throughout all steps. Having learned this, we now have a justification for applying further data filtering criteria.

#### Periodic data

The seasons are an important axis of variation in our data. Hence our data are periodic - with a periodicity of 365 days in the `ddf` dataset and with both 12 hours and 365 days in the `hhdf` dataset. A polar coordinate system, instead of a cartesian system, lends itself to displaying periodic data. A polar coordinate system reflects the fact that, for example, January 1st is closer to December 31st, although they are located on the extreme end of a linear spectrum of days in a year. In a polar coordinate system, the x-axis spans the angle (360$^\circ$, like the clock hands), while the y-axis spans the radius (distance from the center). This is specified by changing the coordinate system of the ggplot object by `+ coord_polar()`.

Below, we first aggregate the data to get a mean seasonal cycle from `ddf` (a, b), and to get a mean diurnal (daily) cycle from June data in `hhdf` (c, d). To get the mean seasonal cycle, we first determine the day-of-year (counting from 1 for January first to 365 for December 31st) using the lubridate function `yday()`. 

```{r}
gg1 <- ddf |>
  mutate(doy = yday(date)) |> 
  group_by(doy) |> 
  summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |> 
  ggplot(aes(doy, GPP_NT_VUT_REF)) + 
  geom_line()

gg2 <- ddf |>
  mutate(doy = yday(date)) |> 
  group_by(doy) |> 
  summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |> 
  ggplot(aes(doy, GPP_NT_VUT_REF)) + 
  geom_line() +
  coord_polar()

gg3 <- hhdf |>
  mutate(month = month(TIMESTAMP_START)) |> 
  filter(month == 6) |>  # taking only June data
  mutate(hour = hour(TIMESTAMP_START)) |> 
  group_by(hour) |> 
  summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |> 
  ggplot(aes(hour, GPP_NT_VUT_REF)) + 
  geom_line()

gg4 <- hhdf |>
  mutate(month = month(TIMESTAMP_START)) |> 
  filter(month == 6) |>  # taking only June data
  mutate(hour = hour(TIMESTAMP_START)) |> 
  group_by(hour) |> 
  summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF)) |> 
  ggplot(aes(hour, GPP_NT_VUT_REF)) + 
  geom_line() +
  coord_polar()

plot_grid(gg1, gg2, gg3, gg4, ncol = 2, labels = "auto")
```

#### Density along two continuous variables

Scatter plots can appear overcrowded. In this example, particularly in the low PPFD range, many points are plotted over each other, which may hide some information. To avoid obscuring important details in the plot, we may want to visualise the *density* of points. We want to plot how many points fall within bins of a certain range values in GPP and PPFD, this creates grid cells in the GPP-PPFD-space. We can create such a raster plot that measures the density using `stat_density_2d()`:

```{r}
ddf |>
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +
  stat_density_2d(
    geom = "raster", #the geometric object to display the data (in this case: rectangles)
    aes(fill = after_stat(density)), #using `density`, a variable calculated by the stat
    contour = FALSE 
    ) +
  scale_fill_viridis_c() +
  labs(x = expression(paste("PPFD (", mu, "mol m"^-2, "s"^-1, ")")), 
       y = expression(paste("GPP (gC m"^-2, "s"^-1, ")")) ) +
  theme_classic() + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))

ddf |>
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +
  geom_hex() +
  scale_fill_viridis_c() +
  labs(x = expression(paste("PPFD (", mu, "mol m"^-2, "s"^-1, ")")), 
       y = expression(paste("GPP (gC m"^-2, "s"^-1, ")")) ) +
  theme_classic() + 
  scale_x_continuous(expand = c(0, 0)) +
  scale_y_continuous(expand = c(0, 0))
```

#### Response surfaces and raster data





In our intial research question, we want to know not only about variations in GPP, but also what controls it. We want to know the environmental factors that determine the variations in GPP. The environmental factors that influence GPP are known as the covariates of GPP. In a machine learning context, we call them "predictors" or "features". To answer this question, we will have to turn to modelling. Here, we refer to modelling in the wider sense of predicting observed variations in a target variable based on empirical relationships with a set of predictors. Often, you will start delving into your research question with some *a priori* understanding of the system from which you have observational data. Such an understanding may be informed by previous observations and their interpretations, or by theory. In middle school already we learnt that photosynthesis requires sunlight and it shouldn't come as a surprise that the more sunlight there is in a day, the higher the GPP. Such a presumed positive (maybe even monotonically increasing) relationship is also consistent with the apparent agreement between the scales of variation in GPP and the scales of variation in incoming solar radiation (dark night, bright day; dark winter, bright summer).

In our dataset, `SW_IN_F` is the incoming photosynthetic photon flux density, measured in mol photons (that come in the right wavelength to be used for photosynthesis). We can plot this relationship to vizualise how it correlates with GPP using the daily data.

```{r}
ddf |>
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +
  geom_point() +
  labs(x = expression(paste("PPFD (", mu, "mol m"^-2, "s"^-1, ")")), y = expression(paste("GPP (gC m"^-2, "s"^-1, ")")) )
```

We observe a clear trend of increasing GPP with increasing PPFD, and it looks largely linear. Data collected in the field often has a substantial amount of scatter. Since we previously added a column to the data frame containing the data quality, the next step is to see if the data quality explains some of the scatter in the data. To investiagte this, we can "map" the data quality dimension onto the color aesthetic of the plot.

```{r}
ddf |>
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = f_measured)) +
  geom_point() +
  scale_color_viridis_c(direction = -1) +
  labs(x = expression(paste("PPFD (", mu, "mol m"^-2, "s"^-1, ")")), y = expression(paste("GPP (gC m"^-2, "s"^-1, ")")) )
```

We can notice that the colours showing the data quality or `f_measured` are mixed across the range of the values for GPP and PPFD. The high GPP values, we previously found to be associated with low fractions of underlying measured data in the time series plot, are not explained by simultaneously high PPFD. In the plot, we see a core cluster of points in the centre forming a positive trend with fewer points outside this denser 'core' area. These outlying points are lighter and do not fit the linear relationship as well.

Despite scattered data points, there is a positive linear relationship between GPP and PPFD. To find the best fit of this linear relationship, meaning the straight line that best fits our data points, we will move on to modelling using a univariate linear regression. The section below, will serve as a brief introduction, since later chapters will go into more detail on modelling and machine learning.

We start by making a simple linear model using the function `lm()` and adding in our desired variables:

```{r}
linmod <- lm(GPP_NT_VUT_REF ~ SW_IN_F, data = ddf)
```

We can also directly plot the fitted linear regression line over the scatter plot using `geom_smooth(method = "lm")`. Rather than first making a linear model and then plotting it onto the data.

```{r}
ddf |>
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +
  geom_point() +
  geom_smooth(method = "lm", color = "red") +
  ylim(-10, 25) +
  labs(x = expression(paste("PPFD (", mu, "mol m"^-2, "s"^-1, ")")), y = expression(paste("GPP (gC m"^-2, "s"^-1, ")")) ) 
```

In red in the plot above is the linear regression fitted directly to our data using `geom_smooth()`, without needing an additional step of first making a linear model and then plotting it over the data.

Based on our previous finding that the data quality is associated with GPP values, which is reflected in their relationship with PPFD, we can fit separate linear regression models for data where `f_measured` is greater than versus less than 0.5. For this, we can create a new variable with `mutate(more_measured = as.factor(f_measured > 0.5))`). The new variable `more_measured` contains binary information as the data is either greater or less than 0.5. By adding new factors we can add another previously "hidden" dimension to our data. And because it is a categorical variable and not a continuous one, R treats it as a *factor*. We can plot this new variable `more_measured` onto the color aesthetic as we did before. Since we specify this aesthetic below in the `ggplot()` function call, all subsequent visualisation layers will respect it, also `geom_smooth()`.

```{r}
ddf |>
  mutate(more_measured = as.factor(f_measured > 0.5)) |> 
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF, color = more_measured)) +
  geom_point(alpha = 0.2) +   # set opacity to 20% to avoid underscernible overplotting
  geom_smooth(method = "lm")  +
  labs(x = expression(paste("PPFD (", mu, "mol m"^-2, "s"^-1, ")")), y = expression(paste("GPP (gC m"^-2, "s"^-1, ")")) )  +
  ylim(-10, 25)
```

We observe a slight difference in the slopes of the respective linear models. Note also that in the above `ggplot()` call, we specified the aesthetics as `aes(x = PPFD_IN, y = GPP_NT_VUT_REF, color = more_measured)`. This triggers all subsequent additions of visualisation layers (here: `geom_piont()` and `geom_smooth`) to use the same aesthetics for plotting. The distinction by the same colors is applied both to the points and to the smoothing lines.


### Advanced data visualisation

Let's dive a bit further into visualisation. We've encountered how to map different dimensions of our data not only onto x and y axes in a cartesian coordinate system, but also along other "aesthetics". We mapped it to color, but there are also other aesthetics available for such mapping in ggplot2, in particular for categorical variables (e.g., line type, point type). Yet another "mapping" is available with `facet_wrap()`. It separates the visualisation into different sub-plots, each showing a part of the data. It's not dealt the same way as other aesthetics (not with specifying it with `aes()`), but with adding the `facet_wrap()` with a `+` to the `ggplot()` object.

Remember that our data frame `ddf_allsites` has the time series data nested for each site in the column `data`. ggplot doesn't like that. Therefore, we can simply `unnest()` the respective column and get a long flat data frame again. Let's plot GPP versus PPFD in separate subplots for each site. The factor (column name) by which `facet_wrap()` separates the plot has to be specified as an argument with a preceeding `~`. Here, this is `~siteid`.

```{r}
ddf_allsites_joined |>
  ggplot(aes(x = SW_IN_F, y = GPP_NT_VUT_REF)) +
  geom_point(alpha = 0.1) +
  facet_wrap(~siteid)
```

What do you notice? Does the relationship look similar for all sites?

Again, we get further if we have some *a priori* knowledge about the system that generated out data. Of course, PPFD just measures the incoming photosynthetically active light. We don't know whether there are any leaves on the trees to absorb that light. While some trees shed their leaves during winter or during the dry period, others are evergreen and the fraction of PPFD they actually absorb doesn't vary by far not as much as for deciduous trees. Therefore, whether a site is dominated by deciduous or evergreen vegetation should somehow affect how directly GPP is correlated with GPP. So there's our hypothesis: Sites with evergreen vegetation should exhibit a higher *R*<sup>2</sup> between GPP and PPFD than sites with deciduous vegetation. Remember that we have evaluated the *R*<sup>2</sup> between GPP and PPFD already above and have it stored as a column `rsq` of our nested data frame `ddf_allsites`. We have also collected site meta-information about the vegetation type (it's in column `lai_fpar`), and have joined the meta info onto the data. Unfortunately, it's not quite ready to use. Since we need a `TRUE`/`FALSE` type of information of whether the vegetation type is evergreen or not, we have to take another step. Below, we take the strings of column `lai_fpar` and check whether it contains the sub-string `"Evergreen"`, using the function `str_detect()` from the stringr package (part of the tidyverse). This boolean information makes up a new column that we call `evergreen`. Sounds like a lot of steps, but in the tidyverse, the code is short and clean:

```{r}
gg <- ddf_allsites_nested_joined |>
  mutate(evergreen = str_detect(igbp_land_use, "Evergreen")) |>
  ggplot(aes(y = rsq, x = evergreen)) +
  geom_boxplot()
print(gg)
```

Eureka II! Indeed, the *R*<sup>2</sup> between GPP and PPFD tend to be higher for sites with evergreen vegetation than for sites with deciduous vegetation. What we see here is a boxplot. It visualises the distribution of *R*<sup>2</sup> by factors (`evergreen`). The fat horizontal like inside the box indicates the median, the lower and upper margins of the box are the 25% and 75% quantiles, the whiskers (vertical lines) extend 1.5 times the inter-quartile range, starting at the upper and lower margins of the box, or just to the smallest and largest values if they are within a shorter distance. Apparently, in our plot above, they are.

Boxplots are useful because they provide a visualisation of distrubtions that can be based on a very large number of underlying points. This yields "light" and clean figures. However, they also hide a lot of information and sometimes it's ok to show more information without overloading the plot. For example, individual points can be shown in the plot above by `geom_jitter()`. Of course, the x-axis represents the level of the factor `evergreen` and when using a simple `geom_points`, all points would be located either at the position (given by the tick mark) of `TRUE` or `FALSE`. This would result in overplotting and thus hiding a large number of points. `geom_jitter()` takes care of this by slightly rearranging them in a random fashion along the x-axis. Note that the rearranged position along the x-axis doesn't actually encode any information. However, as long as points are still separable along the "`evergreen`-axis", this is acceptable.

```{r}
gg + geom_jitter(color = "grey50", width = 0.1)
```

Note that in the two cells above, we have first stored the output of a `ggplot()` call as a new object that we named `gg`. Doing `print(gg)` just creates the respective plot. The advantage is that we can add visualisation elements or modify the theme or add labels to that object in a separate step.

Maybe you've asked yourself whether the magnitude of GPP varies much across vegetation types (column `igbp_land_use` in our dataset).

Let's do another boxplot - because we can...

```{r}
ddf_allsites_joined |>
  ggplot(aes(y = GPP_NT_VUT_REF, x = igbp_land_use)) +
  geom_boxplot() +
  coord_flip()
```

Here, you can see a number of points that are beyond the quartiles plot 1.5 times the inter-quartile range. They are plotted individually and are referred to as "outlying points". We have also flipped the boxes to extend vertically. `coord_flip()` overrides what is specified by `aes()` in that it rotates it by 90<sup>º</sup>.

Before we finish, let's save our nice daily data frame, after having selected interesting variables, having nested data by sites, and after having complemented it with site meta information. We may use it later in the course ...

Since `ddf_allsites_nested_joined` is no longer a "flat" table (it's nested), we cannot save it in a plain text-based format like CSV. Instead, we save it as an R object. In can be later loaded into an R session by `load()`.

```{r}
save(ddf_allsites_nested_joined, file = "data/ddf_allsites_nested_joined.RData")
```

```{r include=FALSE}
## save cleaned daily data frame for ch-lae only
ddf_ch_lae <- ddf_allsites_nested_joined |>
  filter(siteid == "CH-Lae") |>
  select(data) |>
  unnest(data)

save(ddf_ch_lae, file = "data/ddf_ch_lae.RData")
```

A comprehensive

If you are looking for a more complete tutorial on data visualisation which includes exercises and contains information on how to colour specific points, change the shape of certain data points, add different sized points depending on a factor from the data or to add several plots with `facet_wrap()` go to [this link](https://r4ds.had.co.nz/data-visualisation.html). For those who want an in-depth, detailed explanation of many different data visualisation options or how to *"make visualizations that accurately reflect the data, tell a story, and look professional"* go to the following [guide book](https://clauswilke.com/dataviz/) [@wilke_fundamentals_2019].

## Exercises

-   plot `airquality` data along a time axis specified

## Solutions
