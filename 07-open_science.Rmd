# Open science practices {#open_science}

**Chapter lead author: Koen Hufkens**

Contents:

-   Lecture (Koen): Open science - history, motivation, reproducibility crisis, current initiatives, overview of practices
-   Environmental data repositories
-   Methods to create visualised reproducible workflow ???
-   RMarkdown files
-   Performance assessment: **CAT 3**, [link to Dietze exercise on pair coding](https://github.com/stineb/EF_Activities/blob/master/Exercise_04_PairCoding.Rmd)

## Introduction

The scientific method relies on repeated testing of hypothesis. When dealing with data and formal analysis one can reduce this problem to the question: could an independent scientist attain the same results given the data and code.

Although this seems trivial this issue has vexed the scientific community. These days, many scientific publications are based on complex analysis with often large data sets. More so, methods in publications often are insufficiently detailed to really capture the scope of an analysis. Even from this technical point of view, the reproducibility crisis, or the inability to reproduce experimental results, is a complex problem.

This is further compounded by social aspects and incentives. The last decades scientific research has seen a steady increase in speed due to the increasing digitization of many fields and the comodification of science.

Although digitization has opened up new research possibilities it also had less desired outcomes such as the reproducibility crisis, overall decreasing research quality, and outright fraud. Historically research (output) has been confined to academic journals with a limited reach into the public (civil) domain. Digitzation made both the academic output visible, as well as the issues that stem from it (e.g. fraud). In many ways the reproducibility crisis as digital tools have made this position untenable.

Open and reproducible science is in part a counter movement to make scientific research (output) accessible to the larger public, increase research transparency and countering accusations of fraud and limiting disinformation (to some extent). Open science therefore aims to be as open as possible about the whole scientific process, and as closed as desirable.

It is important to acknowledge that there is a spectrum of reproducible data and code workflows which depends on the state or source of the data and the output of the code (or analysis). Within the context of this course and the discussion of open science we focus solely on practices for reproducible science, i.e. ensuring that given the same data and code the results will be similar.

<!-- ![](https://the-turing-way.netlify.app/_images/reproducible-matrix.jpg) -->

For further reading on the topic of reproducibility from an epistemic, or the science of doing science, point of view and general open science practices facilitating other reproducibility modes as shown in the table above I refer to the reference list at the end of this section.

## Project structure

File names??? good vs. bad

R projects

Reproducible science relies on a number of key components. Data management and the tracking of required meta-data is the first step in an open science workflow. Although current computers make it easy to "find" your data and are largely file location agnostic this is not the case in many research environments. Here files need a precise, structured, location. This structure allows you to determine both the function and or order of a workflow.

It is good practice to have a consistent project structure within and between projects. This allows you to find most project components regardless of when you return to a particular project. Structure a project in one folder also makes projects portable. All parts reside in one location making it easy to create a github project from this location, or just copy the project to a new drive.

An example data structure for raw data processing is given below and we provided an [R project template](https://github.com/computationales/R_proj_template) to work from and adjust on the lab website.

``` bash
data-raw/
├─ raw_data_product/
├─ 00_download_raw_data.R
├─ 01_process_raw_data.R
```

## Managing workflows

Although some code is agnostic to the order of execution many projects are effectively workflows, where the output of one routine is required for the successful execution of the next routine.

In order to make sure that a future you, or a collaborator, understands in which order things should be executed it is best to number scripts / code properly. This is the most basic approach to managing workflows.

In the example below all statistics code is stored in the statistics folder in an overall analysis folder (which also includes code for figures). All statistical analysis are numbered, to ensure that the output of a first analysis is available to the following one.

``` bash
analysis/
├─ statistics/
│  ├─ 00_random_forest_model.R
│  ├─ 01_random_forest_tuning.R
├─ figures/
│  ├─ global_model_results_map.R
│  ├─ complex_process_visualization.R
```

### Automating and visualizing workflows with targets

To sidestep some of the manual management in R you can use dedicated pipeline tool like the **{targets} package** in R. The targets package learns how your pipeline fits together, skips tasks that are already up to date, runs only the necessary computation. Given the highly controlled environment {targets} can also visualize the (progress of) your workflow.

<!-- ![](https://books.ropensci.org/targets/man/figures/tar_watch.png) -->

Due to the added complexity of the targets package we won't include extensive examples of such a workflow but refer to the excellent documentation of the package for simple examples (TBD?).

<https://books.ropensci.org/targets/walkthrough.html>

### Self-contained environments

Although R projects and the use of targets make your workflow consistent the package versions used between various systems (e.g. your home computer, a cluster at the university etc. might vary). To address issues with changes in the versions of package you can use the [**{renv} package**](https://rstudio.github.io/renv/) which manages package version (environments) for you. The {renv} package creates a snapshot of your system and allows you to recreate this state elsewhere.

When tasks are even more complex and include components outside of R you can use [**Docker**](https://www.docker.com/) to provide containerization of an operating system and the included application. You can therefore emulate the state of a machine independent of the machine on which the docker file is run. These days Machine Learning applications are often deployed as docker sessions to limit the complexity of installing required software components.

## Rmarkdown

Within Rstudio you can use Rmarkdown dynamic documents to combine both text and code. Rmarkdown is ideal for **reporting** i.e., writing your final document presenting your analysis results. A Rmarkdown documents consists of a header document properties, such as how it should be rendered (as an html page, a docx file or a pdf), and the actual content.

Below you see the header file of an Rmarkdown document that should be rendered as an html page.

``` r
---
title: My R Markdown Report
author: You
output: html_document
---
```

The remainder of the document shows a code chunk outlined by \`\`\` quotes and the chunk arguments in {} brackets. Inline operations, the evaluation of code, in text is also possible by using single quotes around a variable or code.

```` bash
``` {r}
x <- 5  # radius of a circle
```

For a circle with the radius `r x`,
its area is `r pi * x^2`.
````

The document can be rendered by calling `rmarkdown::render()` on the command line or hitting the "Knit" button in the RStudio IDE. Depending on your settings a html file, pdf or docx file will be generated in your current directory (and or displayed in the IDE viewer).

#### Referencing and finding files

In R projects all files can be referenced relative to the top most path of the project. When opening `your_project.Rproj` in RStudio you can load data in the console as such `read.table("data/some_data.csv")`, specifying a "soft" relative path for `some_data.csv`.

``` bash
project/
├─ your_project.Rproj
├─ statistics/
│  ├─ your_dynamic_document.Rmd
├─ data/
│  ├─ some_data.csv
```

Rmarkdown files are rendered relative to the file path where to document resides. This means that data which resides in `data` can't be accessed by `statistics/your_dynamic_document.Rmd` even when using an R project and soft relative paths (which work for scripts and functions). As such trying to render the `your_dynamic_document.Rmd` will fail as the file `some_data.csv` will not be found.

```` bash
---
title: Your dynamic document
author: You
output: html_document
---

```{r eval = FALSE}
data <- read.table('data/some_data.csv')
```
````

We need to explicitly take the project's base path into the fold using the [{here} package](https://here.r-lib.org/). The here package gathers the absolute path of files inside an R project. As such, `here::here("data/some_data.csv")` will return the full path of the data (e.g. `/your_computer/project/data/some_data.csv`). This absolute path will be a valid path for the read.table() function, making the Rmarkdown file render correctly. The correct Rmarkdown code to read the data therefore reads:

```` bash
---
title: Your dynamic document
author: You
output: html_document
---

```{r eval = FALSE}
data <- read.table(here::here('data/some_data.csv'))
```
````

But why not use absolute paths to begin with? **Portability**! When I would run your \*.Rmd file with an absolute path on my computer it would not render as the file `some_data.csv` would then be located at: `/my_computer/project/data/some_data.csv`

#### Limitations

The file referencing issue and the common use of Rmarkdown as a one size fits all solution, containing all aspects from data cleaning to reporting, makes Rmarkdown files are rarely portable or reproducible. This one size fits all approach to Rmarkdown also encourages bad project management practices. As illustrated above, if no session management tools such as the package {here} are used this automatically causes files to pile up in the top most level of a project, undoing most efforts to physically structure data and code.

At the heart of this discussion are not only practical considerations but also the fact that R markdown documents mix two cognitive tasks, writing text (reporting, not commenting code) and writing code. Switching between these two modes comes with undue overhead. If you code, you should not be writing prose, and vise versa.

If your R markdown file contains more code than it does text, it should be considered an R script or function (with comments or documentation). Conversely, if your markdown file contains more text than code it probably is easier to collaborate on a true word processing file (or cloud based solution). The use case where the notebooks might serve some importance is true reporting of general statistics, or when using consistent git(hub) version control.

R markdown files have their function for reporting concise results, once generated (through functions or analysis scripts) but should be generally be avoided to develop code / ideas (see cognitive switching remark)!

## Learning objectives

## Exercises

### Data and code management

#### External data

You are inherit a project folder which contains the following files.

``` bash
/home/xyz/project/
├─ survey.xlsx
├─ xls conversion.csv
├─ xls conversion (copy 1).csv
├─ Model-test_1.R
├─ Model-test-final.R
├─ Plots.R
├─ Figure 1.png
├─ test.png
├─ Rplot01.png
├─ Report.Rmd
├─ Report.html
```

What are your steps to make this project more reproducible? Write down how and why you would organize your project.

#### A new project

What are the basic steps to create a reproducible workflow from a file management perspective? Create your own project using these principles. Provide details the on steps involved and why they matter.

The project should a reproducible workflow:

-   to download and plot a MODIS land cover map for Belgium

-   contain a function to count the occurences of land cover classes in the map as a formal function

-   create a plot of the land cover map

-   contain a dynamic report describing your answers to the above questions regarding how to structure a reproducible workflow

#### Tracking your project

Can you track the state of your project using {targets}?

-   create a simple targets project using the above workflow

    -   make changes to the API download routine

    -   rerun the targets project

## Solutions

## References

-   complete references
