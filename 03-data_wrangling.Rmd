# Data wrangling {#data_wrangling}

**Chapter lead author: Benjamin Stocker**

Contents:

-   Exploratory data analysis, "understanding the data"

-   Tabular data

-   Read in the example data, tidyverse

-   Tidy data, variables

    -   Selecting variables by their names: `select()`
    -   Renaming: `rename()`

-   Dimensions of variation

-   Dealing with missingness, bad data, outliers, gap-filling (distinguished from *imputation* as part of the modelling workflow)

    -   Selecting observations by their values: `filter()`
    -   Creating new variables: `mutate()`

-   Data aggregation:

    -   multiple values down to a single summary: `summarise()`

-   Combining data

    -   `bind_*()`

    -   relational data: `*_join()`

-   More common wrangling

    -   strings (stringr)
    -   time, date (lubridate)
    -   nesting (tidyr)
    -   functional programming (purrr)

-   Performance assessment: **CAT 1,** [link](https://stineb.github.io/esds_book/ch-02.html#exercise-1), make table tidy

## Learning objectives

## Tutorial

Exploratory data analysis - the transformation, visualization, and modelling of data - is the central part of any (geo-) data science workflow and typically takes up a majority of the time we spend on a research project. The transformation of data often turns out to be particularly (and often surprisingly) time-demanding. Therefore, it is key to master typical steps of data transformation, and to implement them efficiently - both in terms of robustness against coding errors ("bugs") and in terms of code execution speed. This chapter introduces typical transformation steps (the reduction, filtering, aggregation, and combination of data), applied to *tabular data*, and implemented using the R [*tidyverse*](https://www.tidyverse.org/) "dialect" (see below).

### Example data

The example data used in this chapter are parallel time series of (gaseous) CO2 and water vapor exchange fluxes between the vegetation and the atmosphere, along with various meteorological variables measured in parallel. Quasi-continuous measurements of temporally changing gas exchange fluxes are measured with the eddy covariance technique which relies on the parallel quantification of vertical wind speeds and gas concentrations.

The data is provided at half-hourly resolution for the site [CH-Lae](https://www.swissfluxnet.ethz.ch/index.php/sites/ch-lae-laegeren/site-info-ch-lae/), located on the south slope of the Lägern mountain on the Swiss Plateau at 689 m a.s.l. in a mixed forest with a distinct seasonal course of active green leaves (a substantial portion of the trees are deciduous). The dataset is generated and formatted following standard protocols ([FLUXNET2015](https://fluxnet.org//data/fluxnet2015-dataset/)). For more information of the variables in the dataset, see the [FLUXNET2015 website](http://fluxnet.fluxdata.org/data/fluxnet2015-dataset/) and [Pastorello et al., 2020](https://www.nature.com/articles/s41597-020-0534-3) for a comprehensive documentation of variable definitions and methods.

For our demonstrations most relevant are the following variables:

-   `TIMESTAMP_START`: Hour and day of the start of the measurement period for which the respective row's data is representative. Provided in a format of "YYYYMMDDhhmm".
-   `TIMESTAMP_END`: Hour and day of the end of the measurement period for which the respective row's data is representative. Provided in a format of "YYYYMMDDhhmm".
-   `TA_*` (°C): Air temperature.
-   `SW_IN_*` (W m$^{-2}$): Shortwave incoming radiation
-   `LW_IN_*` (W m$^{-2}$): Longwave incoming radiation
-   `VPD_*` (hPa): Vapor pressure deficit (the difference between actual and saturation water vapor pressure)
-   `PA_*` (kPa): Atmospheric pressure
-   `P_*` (mm): Precipitation
-   `WS_*` (m $^{-1}$): Wind speed
-   `SWC_*` (%): Volumetric soil water content
-   `GPP_*` ($\mu$mol CO$_2$ m$^{-1}$ s$^{-1}$): Gross primary production (the ecosystem-level gross CO2 uptake flux driven by photosynthesis)
-   `*_QC`: Quality control information for the variable `*`. Important for us: `NEE_*_QC` is the quality control information for the net ecosystem CO2 exchange flux (`NEE_*`) and for GPP derived from the corresponding NEE estimate (`GPP_*`). 0 = measured, 1 = good quality gap-filled, 2 = medium, 3 = poor.

Suffixes `_*` indicate that multiple estimates for respective variables are available and distinguished by different suffixes. For example, variables `TA_*` contain the same information, but are derived with slightly different assumptions and gap-filling techniques. The meanings of suffixes are described in [Pastorello et al., 2020](https://www.nature.com/articles/s41597-020-0534-3).

### Required libraries

Install missing packages and load all required packages for this tutorial.

```{r message=FALSE, warning=FALSE}
use_pkgs <- c("dplyr", "tidyr", "readr", "lubridate", "stringr", "purrr")
new_pkgs <- use_pkgs[!(use_pkgs %in% installed.packages()[, "Package"])]
if (length(new_pkgs) > 0) install.packages(new_pkgs)
invisible(lapply(use_pkgs, require, character.only = TRUE))
```

### Tidyverse

The tidyverse is a collection of R packages and functions that share a common design philosophy, enabling a particularly efficient implementation of transformation steps on tabular data. The most important data and function design principle of the tidyverse is that each function takes a data frame as its first argument and returns a data frame as its output.

<!-- Working on a (geo-) data science project, we want to efficiently progress through the multiple circles of exploratory data analysis. The following aspects are particularly important for fast and error-free progression. First, the programming language should be conducive of fast, intuitive, and error-free coding. Second, code that we have written once should be legible by others and by our future selves (even after a long holiday or after the manuscript has been seen by all reviewers months after we finished the analysis for our initial submission). It's the daily reality of us (geo-) data scientist that the code we write is harder to read than text in a newspaper, and that it will have bugs. There is (so far) no magic solution to this. But the R tidyverse comes close.  -->

From this design principles, even the most convoluted code and implementation of data transformation steps fall into place and fast and error-free progression through exploratory data analysis is facilitated. Therefore, you will be introduced to the R tidyverse here and we heavily rely on this dialect of the R language throughout the remainder of this course.

### Tabular data

Tabular data is organised in rows and columns. Each column can regarded as a vector of a certain type. Each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). Each row can be regarded as a separate instance of the same type, for example a record of simultaneously taken measurements, along with some attributes. For example, a data frame in R is tabular data. In Chapter \@ref(data_variety), you will be introduced to other types of data. The most common format for tabular data is CSV (comma-separated-values), typically indicated by the file name suffix `.csv`. CSV is a text-based file format, readable across platforms and does not rely on proprietary software (as opposed to, for example, `.xlsx`). The first row in a CSV file typically specifies the name of the variable provided in the respective column.

Let's get started with working with our example data set and read it into R, as the variable `hhdf`.

```{r}
hhdf <- read_csv("./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv")
hhdf
```

Since the file is properly formatted, with variable names given in the first line of the file, the function `read_csv()` identifies them correctly as column names and interprets values in each column as values of a consistent type. We used the function `read_csv()` from the **readr** package (part of tidyverse) here for reading the CSV since it is faster than the base-R `read.csv()` and generates a nicely readable output when printing the object as is done above.

### Variable selection

For our further data exploration, we will reduce the data frame we are working with and select a reduces set of variables. Reducing the dataset can have the advantage of speeding up further processing steps, especially when the data is large. For the further steps in this chapter we will now subset our original data. We select the following variants of variables described above, plus some additional variables (further information in [Pastorello et al., 2020](https://www.nature.com/articles/s41597-020-0534-3)):

-   All variables with names starting with `TIMESTAMP`)
-   All meteorological variables derived following the "final gap-filled method", as indicated with names ending with `_F`.
-   GPP estimates that are based on the nighttime decomposition method, using the "most representative" of different gap-filling versions, after having applied the variable u-star filtering method (`GPP_NT_VUT_REF`) and the corresponding quality control information (`NEE_VUT_REF_QC`)
-   Soil water measured at different depths (variables starting with `SWC_F_MDS_`)
-   Do not use any radiation variables derived with the "JSBACH" algorithm (not with a name that contains the string `JSB`)
-   Flag indicating whether a time step is at night (`NIGHT`)

This is implemented by:

```{r}
hhdf <- select(
  hhdf,
  starts_with("TIMESTAMP"),
  ends_with("_F"),
  GPP_NT_VUT_REF,
  NEE_VUT_REF_QC,
  starts_with("SWC_F_MDS_"),
  -contains("JSB"),
  NIGHT
  )
```

This reduces our dataset from 235 available variables to 59 variables. Our data set now only contains the columns we will need in our further analysis. As you can see, `select()` is a powerful tool to apply multiple selection criteria on your data frame in one step. It takes many functions that make filtering the columns easier. For example, criteria can be formulated based on the variable names with `starts_with()`, `ends_with`, `contains()`, `matches()`, etc. Using these functions within `select()` can help if several column names start with the same characters or contain the same pattern and all need to be selected. If a minus (`-`) is added in front of a column name or one of the mentioned functions within `select()`, then R will not include the stated column(s). Note that the selection criteria are evaluated in the order we write them in the `select()` function call. You can find the complete reference for selecting variables [here](https://dplyr.tidyverse.org/reference/select.html).

### Time objects

The automatic interpretation of the variables `TIMESTAMP_START` and `TIMESTAMP_END` by the function `read_csv()` is not optimal:

```{r}
class(hhdf$TIMESTAMP_START[[1]])
as.character(hhdf$TIMESTAMP_START[[1]])
```

As we can see, it is considered by R as a numeric variable with 12 digits ("double-precision", occupying 64 bits in computer memory). After printing the variable as a string, we can guess that the format is: YYYYMMDDhhmm. The **lubridate** [@R-lubridate] package is designed to facilitate processing date and time objects. Knowing the format of the timestamp variables in our dataset, we can use `ymd_hm()` to convert them to actual date-time objects.

```{r, message=FALSE}
dates <- ymd_hm(hhdf$TIMESTAMP_START)
dates[1]
```

Working with such date-time objects facilitates typical operations on time series. For example, adding one day can be done by:

```{r}
nextday <- dates + days(1)
nextday[1]
```

The following returns the month of each date object:

```{r}
month(dates[1])
```

The number 1 stands for the month of the year, i.e., January. You can find more information on formatting dates and time within the **tidyverse** [here](https://r4ds.had.co.nz/dates-and-times.html), and a complete reference of the **lubridate** package is available [here](https://lubridate.tidyverse.org/).


### Variable (re-) definition

Since `read_csv()` did not interpret the `TIMESTAMP_*` variables as desired, we may convert the entire column in the data frame into a date-time object. In base-R, we would do this by:

```{r eval=FALSE}
hhdf$TIMESTAMP_START <- ymd_hm(hhdf$TIMESTAMP_START)
```

Modifying existing or creating new variables (columns) in a data frame is done in the tidyverse using the function `mutate()`. The equivalent statement is:

```{r eval=FALSE}
hhdf <- mutate(hhdf, TIMESTAMP_START = ymd_hm(TIMESTAMP_START))
```

Note that in the code chunk above, the function `mutate()` is from the tidyverse package *dplyr*. It takes a dataframe as its first argument (here `hhdf`) and returns a dataframe as its output. You will encounter an alternative, but equivalent, syntax in the following form:

```{r, eval=FALSE}
hhdf <- hhdf |> 
  mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START))
```

Here, the pipe operator `|>` is used. It "pipes" the object evaluated on its left side into the function on its right side, where the object takes the place of (but is not spelled out as) the first argument of that function. Using the pipe operator can have the advantage of facilitating the separation, removal, inserting, or re-arranging of individual transformation steps. Arguably, it facilitates reading code, especially for complex data transformation workflows. Therefore, you will encounter the pipe operator frequently throughout the remainder of this course.

Mutating both our timestamp variables could be written as `mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START), TIMESTAMP_END = ymd_hm(TIMESTAMP_END))`. Sometimes, such multiple-variable mutate statements can get quite long. A handy short version of this can be implemented using `across()`:

```{r}
hhdf <- hhdf |> 
  mutate(across(starts_with("TIMESTAMP_"), ymd_hm))
```

We will encounter more ways to use mutate later in this tutorial. A complete reference to `mutate()` is available [here](https://r4ds.had.co.nz/transform.html#add-new-variables-with-mutate).


### Axes of variation

Tabular data is two-dimensional (rows $\times$ columns), but not all two-dimensional data is tabular. In Chapter \@ref(data_variety), you will encounter raster data - a two-dimensional array of data (a matrix) representing variables on an evenly spaced grid, for example pixels in remotely sensed imagery. For example the `volcano` data (provided as an example dataset in R) is a 2-dimensional array, each column contains the same variable, and no variable names are provided. 
```{r}
dim(volcano)
volcano[1:5, 1:5]
```
In the `volcano` dataset, rows and columns represent different geographic positions in latitude and longitude, respectively. The `volcano` data is not tabular data. Another typical example for non-tabular data are climate model outputs. They are typically given for each pixel along a longitudinal, latitudinal, and vertical axis, and for multiple time steps. Such data is four-dimensional and, as such, definitely not tabular.

However, tabular data, although formatted in two dimensions by rows and columns, may represent data that varies along multiple axes. Our example data contains values recorded at each half-hourly time interval over the course of eleven years (check by `nrow(hhdf)/(2*24*365)`). The data is recorded at a site, located in the temperate climate zone, where solar radiation and therefore also other meteorological variables and ecosystem fluxes vary substantially over the course of a day and over the course of a year. Although not explicitly separated, the date-time object thus encodes information along multiple *axes of variation* in the data. For example, over the course of one day (`2*24` rows in our data), the shortwave incoming radiation `SW_IN_F` varies over a typical diurnal cycle:

```{r}
plot(hhdf[1:(2*24),]$TIMESTAMP_START, hhdf[1:(2*24),]$SW_IN_F, type = "l")
```

Over the course of an entire year, shortwave incoming radiation varies with the seasons, peaking in summer:

```{r}
plot(hhdf[1:(365*2*24),]$TIMESTAMP_START, hhdf[1:(365*2*24),]$SW_IN_F, type = "l")
```

All data frames have two dimensions, rows and columns. Our data frame is organised along half-hourly time steps in rows. As described above, these time steps belong to different days, months, and years, although these "axes of variation" are not reflected by the structure of the data frame and we do not have columns that indicate the day, month or year of each half-hourly time step. This would be redundant information since the date-time objects of columns `TIMESTAMP_*` contain this information. However, for certain applications, it may be useful to separate information regarding these axes of variation more explicitly. For example by:

```{r}
hhdf |>
  mutate(year = year(TIMESTAMP_START),
         month = month(TIMESTAMP_START)) |>
  select(TIMESTAMP_START, TIMESTAMP_END, year, month)  # for displaying
```

Note that we used `mutate()` here to create a new variable (column) in the data frame, as opposed to above where we overwrote an existing variable with the same function.

Note also that these temporal axes are *hierarchical* (several half-hours within a day, several days within a year). Other examples of such a hierarchical structure of axes of variation are cantons (or provinces or counties) within countries, or (typical for ecological data) individuals within species within *genera* within *families*, or (typical for data collected in an experiment) *sample* within *treatment*.

### Tidy data

Data comes in many forms and shapes. For example, Excel provides a playground for even the wildest layouts of information in tabular form and merged cells as we will see in the Exercises. A data frame imposes a relatively strict formatting in named columns of equal length. But even data frames can come in various shapes - even if the information they contain is the same.

```{r echo=FALSE}
## this is hidden - used to create the objects printed in next chunk
## using the ts object 'co2'
df1 <- tibble(co2_concentration = as.vector(co2), 
       year_dec = time(co2)
       ) |> 
  mutate(month_dec = year_dec - as.integer(year_dec)) |> 
  mutate(year = year_dec - month_dec) |> 
  mutate(month = as.integer(month_dec * 12)) |> 
  mutate(date = ymd(paste0(as.character(year), as.character(month + 1), "-15"))) |> 
  mutate(month = month(date, label = TRUE)) |> 
  select(year, month, co2_concentration) |> 
  slice(1:36)

df2 <- df1 |> 
  pivot_wider(names_from = month, values_from = co2_concentration)

df3 <- df1 |> 
  # mutate(year = as.character(year)) |> 
  pivot_wider(names_from = year, values_from = co2_concentration)
```

```{r}
df1
df2
df3
```

There are advantages for interoperability and ease of use when data frames come with consistent layouts, adhering to certain design principles. We have learned that in tabular data, each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). And that each row can be regarded as a separate instance of the same type, for example a record of simultaneously taken measurements, along with some attributes. Following these principles strictly leads to *tidy data*. In essence, quoting [Wickham](https://r4ds.had.co.nz/tidy-data.html), data is tidy if:

- Each variable has its own column.
- Each observation has its own row.
- Each value has its own cell.

![Rules for tidy data](./fig/tidy_data.png)
<!-- Figure from https://r4ds.had.co.nz/tidy-data.html -->
<!-- Figure URL: https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png -->

The concept of tidy data can even be taken further by understanding a “value” as any object type, e.g. a list or a data frame. This leads to a list or data frame “nested” within a data frame. You will learn more about this below.

The contents of this tutorial are inspired by the (freely available online) book [\*R for Data Science\* by Grolemund & Wickham](%5Bhttps://r4ds.had.co.nz/).](<https://r4ds.had.co.nz/>).)


### Aggregating data

Aggregating data refers to collapsing a larger set of values into a smaller set of values that are derived from the larger set. For example, we can aggregate over all $N$ rows in a data frame ($N\times M$), calculating the sum for each of the $M$ columns. This returns a data frame ($1 \times M$) with the same number of columns as the initial data frame, but only one row. Often, aggregations are done not across all rows but for rows within $G$ groups of rows. This yiels a data frame ($G \times M$) with the number of rows corresponding to the number of groups.

Let's say we want to calculate the mean of half-hourly shortwave radiation within each day. That is, to aggregate our half-hourly data to daily data by taking a mean. There are two pieces of information needed for an aggregation step: The factor (or "axis of variation"), here days, that groups a vector of values for collapsing it into a single value, and the function used for collapsing values, here, the `mean()` function. This function should take a vector as an argument and return a single value as an output. These two steps are implemented by the *dplyr* functions `group_by()` and `summarise()`. This aggregation workflow is implemented by the following code:

```{r message=FALSE}
ddf <- hhdf |>  
  mutate(date = as_date(TIMESTAMP_START)) |>  # converts the ymd_hm-formatted date-time object to a date-only object (ymd)
  group_by(date) |> 
  summarise(SW_IN_F = mean(SW_IN_F))
ddf
```

The seasonal course can now be more clearly be visualized with the data aggregated to daily values.
```{r}
plot(ddf[1:365,]$date, ddf[1:365,]$SW_IN_F, type = "l")
```

More info on how to group values using summarise functions [here](https://r4ds.had.co.nz/transform.html#grouped-summaries-with-summarise), or a summary on the inputs the function [group_by()](https://dplyr.tidyverse.org/reference/group_by.html) and [summarise()](https://dplyr.tidyverse.org/reference/summarise.html) take.

### Data cleaning

Data cleaning is often a time-consuming task and decisions taking during data cleaning may be critical for analyses and modelling. In the following, we distinguish between cleaning formats, the identification (and removal) of "bad" data, and the gap-filling of missing or removed data.

#### Cleaning formats

As a general principle, we want to make our data *machine readable*. Key for achieving machine-readability is that a cell should only contain one value of one type. For example, avoid entries like `>10 m`. Here, we have three pieces of information (`>` as in "greater than", `10`, and `m` indicating the units). A machine-readable format would be obtained by creating separate columns for each piece of information. The `>` should be avoided already at the stage of recording the data. Here, we may have to find a solution for encoding it in a machine readable manner (see Exercises).

#### Bad data

- outliers (sensor error, human error, representing a different population, unsuitable measurement conditions [eddy covariance example, remote sensing example])
- identification via distributions [hist(), boxplot(), qqnorm()], multivariate (Cook's Distance to get influential values), spurious value (large count of numeric values)
- randomly missing or not?
- replace by NA? filter row?
- -> anomaly detection

```{r eval=FALSE}
data <- data[!data %in% boxplot.stats(data)$out]
```

The question about what is "bad data" and whether or when it should be removed is often critical. Such decisions are important to keep track of and should be reported as transparently as possible in publications. In reality, where the data generation process may start in the field with actual human beings writing notes in a lab book, and where the human collecting the data is often not the same as the human analyzing the data or writing the paper, it's often more difficult to keep track of such decisions. As a general principle, it is advisable to design data records such that decisions made during the data collection process remain transparent throughout all stages of the workflow and that sufficient information be collected to enable later revisions of particularly critical decisions.



Several ML algorithms require missing values to be removed. That is, if any of the cells in one row has a missing value, the entire cell gets removed. Data may be missing for several reasons. Some yield random patterns of missing data, others not. In the latter case, we can speak of *informative missingness* (Kuhn & Johnson, 2003) and its information can be used for predictions. For categorical data, we may replace such data with `"none"` (instead of `NA`), while randomly missing data may be dropped altogether. Some ML algorithms (mainly tree-based methods, e.g., random forest) can handle missing values. However, when comparing the performance of alternative ML algorithms, they should be tested with the same data and removing missing data should be done beforehand.

Visualising missing data is essential for making decisions about dropping rows with missing data versus removing predictors from the model (which would imply too much data removal). The cells with missing data in a data frame can be eaily visualised e.g. with `vis_miss()` from the *visdat* package.

```{r warning=FALSE, message=FALSE}
library(visdat)
vis_miss(
  ddf,
  cluster = FALSE, 
  warn_large_data = FALSE
  )
```


#### Gap-filling

tbc

## Exercises

## Solutions
