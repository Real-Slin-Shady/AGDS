# Data wrangling {#datawrangling}

**Chapter lead author: Benjamin Stocker**

## Learning objectives

## Tutorial

Exploratory data analysis - the transformation, visualization, and modelling of data - is the central part of any (geo-) data science workflow and typically takes up a majority of the time we spend on a research project. The transformation of data often turns out to be particularly (and often surprisingly) time-demanding. Therefore, it is key to master typical steps of data transformation, and to implement them in a transparent fashion and efficiently - both in terms of robustness against coding errors ("bugs") and in terms of code execution speed. This chapter introduces typical transformation steps (the reduction, filtering, aggregation, and combination of data), applied to *tabular data*, and implemented using the R [*tidyverse*](https://www.tidyverse.org/) "dialect" (see below).

We refer to data *wrangling* here to encompass the steps for preparing the data set *prior* to modelling - including, the combination of variables from different data sources, the removal of bad data, and the aggregation of data to the desired resolution or granularity (e.g., averaging over all time steps in a day, or over all replicates in a sample).

In contrast, *pre-processing* refers to the additional steps that are either required by the the specific machine learning algorithm used with the data (e.g., centering and scaling for K-Nearest Neighbors or Neural Networks), the gap-filling of variables, or the transformation of variables guided by the resulting improvement of the predictive power of the machine learning model. Pre-processing is part of the modelling workflow and includes all steps that apply transformations that use parameters derived from the data. We will introduce and discuss data pre-processing in Chapter \@ref(supervised_ml).

### Example data

The example data used in this chapter are parallel time series of (gaseous) CO$_2$ and water vapor exchange fluxes between the vegetation and the atmosphere, along with various meteorological variables measured in parallel. Quasi-continuous measurements of temporally changing gas exchange fluxes are measured with the eddy covariance technique which relies on the parallel quantification of vertical wind speeds and gas concentrations.

The data is provided at half-hourly resolution for the site [CH-Lae](https://www.swissfluxnet.ethz.ch/index.php/sites/ch-lae-laegeren/site-info-ch-lae/), located on the south slope of the Lägern mountain on the Swiss Plateau at 689 m a.s.l. in a mixed forest with a distinct seasonal course of active green leaves (a substantial portion of the trees in the measured forest are deciduous). The dataset is generated and formatted following standard protocols ([FLUXNET2015](https://fluxnet.org//data/fluxnet2015-dataset/)). For more information of the variables in the dataset, see the [FLUXNET2015 website](http://fluxnet.fluxdata.org/data/fluxnet2015-dataset/) and [Pastorello et al., 2020](https://www.nature.com/articles/s41597-020-0534-3) for a comprehensive documentation of variable definitions and methods.

For our demonstrations, the following variables are the most relevant:

-   `TIMESTAMP_START`: Hour and day of the start of the measurement period for which the respective row's data is representative. Provided in a format of "YYYYMMDDhhmm".
-   `TIMESTAMP_END`: Hour and day of the end of the measurement period for which the respective row's data is representative. Provided in a format of "YYYYMMDDhhmm".
-   `TA_*` (°C): Air temperature.
-   `SW_IN_*` (W m$^{-2}$): Shortwave incoming radiation
-   `LW_IN_*` (W m$^{-2}$): Longwave incoming radiation
-   `VPD_*` (hPa): Vapor pressure deficit (the difference between actual and saturation water vapor pressure)
-   `PA_*` (kPa): Atmospheric pressure
-   `P_*` (mm): Precipitation
-   `WS_*` (m $^{-1}$): Wind speed
-   `SWC_*` (%): Volumetric soil water content
-   `GPP_*` ($\mu$mol CO$_2$ m$^{-1}$ s$^{-1}$): Gross primary production (the ecosystem-level gross CO$_2$ uptake flux driven by photosynthesis)
-   `*_QC`: Quality control information for the variable `*`. Important for us: `NEE_*_QC` is the quality control information for the net ecosystem CO$_2$ exchange flux (`NEE_*`) and for GPP derived from the corresponding NEE estimate (`GPP_*`). 0 = measured, 1 = good quality gap-filled, 2 = medium, 3 = poor.

Suffixes `_*` indicate that multiple estimates for respective variables are available and distinguished by different suffixes. For example, variables `TA_*` contain the same information, but are derived with slightly different assumptions and gap-filling techniques. The meanings of suffixes are described in [Pastorello et al., 2020](https://www.nature.com/articles/s41597-020-0534-3).

### Tidyverse

The tidyverse is a collection of R packages and functions that share a common design philosophy, enabling a particularly efficient implementation of transformation steps on tabular data. The most important data and function design principle of the tidyverse is that each function takes a data frame as its first argument and returns a data frame as its output.

<!-- Working on a (geo-) data science project, we want to efficiently progress through the multiple circles of exploratory data analysis. The following aspects are particularly important for fast and error-free progression. First, the programming language should be conducive of fast, intuitive, and error-free coding. Second, code that we have written once should be legible by others and by our future selves (even after a long holiday or after the manuscript has been seen by all reviewers months after we finished the analysis for our initial submission). It's the daily reality of us (geo-) data scientist that the code we write is harder to read than text in a newspaper, and that it will have bugs. There is (so far) no magic solution to this. But the R tidyverse comes close.  -->

From this design principles, even the most convoluted code and implementation of data transformation steps fall into place and fast and error-free progression through exploratory data analysis is facilitated. Therefore, you will be introduced to the R tidyverse here and we heavily rely on this dialect of the R language throughout the remainder of this course.

### Tabular data

Tabular data is organised in rows and columns. Each column can be regarded as a vector of a certain type. Each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). Each row can be regarded as a separate instance of the same type, for example a record of simultaneously taken measurements, along with some attributes. For example, a data frame in R is tabular data. In Chapter \@ref(datavariety), you will be introduced to other types of data. The most common format for tabular data is CSV (comma-separated-values), typically indicated by the file name suffix `.csv`. CSV is a text-based file format, readable across platforms and does not rely on proprietary software (as opposed to, for example, `.xlsx`). The first row in a CSV file typically specifies the name of the variable provided in the respective column.

Let's get started with working with our example data set and read it into R, as the variable `hhdf`. Note that the naming of variables can be important for keeping code legible. For example, it may indicate the type of the object it holds (`df` for data frame) and that the data is half-hourly (`hh`).

```{r}
hhdf <- read_csv("./data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006.csv")
hhdf
```

Since the file is properly formatted, with variable names given in the first line of the file, the function `read_csv()` identifies them correctly as column names and interprets values in each column as values of a consistent type. We used the function `read_csv()` from the **readr** package (part of tidyverse) here for reading the CSV since it is faster than the base-R `read.csv()` and generates a nicely readable output when printing the object as is done above.

### Variable selection

For our further data exploration, we will reduce the data frame we are working with and select a reduced set of variables. Reducing the dataset can have the advantage of speeding up further processing steps, especially when the data is large. For the further steps in this chapter we will now subset our original data. We select the following variants of variables described above, plus some additional variables (further information in [Pastorello et al., 2020](https://www.nature.com/articles/s41597-020-0534-3)):

-   All variables with names starting with `TIMESTAMP`)
-   All meteorological variables derived following the "final gap-filled method", as indicated with names ending with `_F`.
-   GPP estimates that are based on the nighttime decomposition method, using the "most representative" of different gap-filling versions, after having applied the variable u-star filtering method (`GPP_NT_VUT_REF`) and the corresponding quality control information (`NEE_VUT_REF_QC`)
-   Soil water measured at different depths (variables starting with `SWC_F_MDS_`)
-   Do not use any radiation variables derived with the "JSBACH" algorithm (not with a name that contains the string `JSB`)
-   Flag indicating whether a time step is at night (`NIGHT`)

This is implemented by:

```{r}
hhdf <- select(
  hhdf,
  starts_with("TIMESTAMP"),
  ends_with("_F"),
  GPP_NT_VUT_REF,
  NEE_VUT_REF_QC,
  starts_with("SWC_F_MDS_"),
  -contains("JSB"),
  NIGHT
  )
```

This reduces our dataset from 235 available variables to 59 variables. As you can see, `select()` is a powerful tool to apply multiple selection criteria on your data frame in one step. It takes many functions that make filtering the columns easier. For example, criteria can be formulated based on the variable names with `starts_with()`, `ends_with`, `contains()`, `matches()`, etc. Using these functions within `select()` can help if several column names start with the same characters or contain the same pattern and all need to be selected. If a minus (`-`) is added in front of a column name or one of the mentioned functions within `select()`, then R will not include the stated column(s). Note that the selection criteria are evaluated in the order we write them in the `select()` function call. You can find the complete reference for selecting variables [here](https://dplyr.tidyverse.org/reference/select.html).

### Time objects

The automatic interpretation of the variables `TIMESTAMP_START` and `TIMESTAMP_END` by the function `read_csv()` is not optimal:

```{r}
class(hhdf$TIMESTAMP_START[[1]])
as.character(hhdf$TIMESTAMP_START[[1]])
```

As we can see, it is considered by R as a numeric variable with 12 digits ("double-precision", occupying 64 bits in computer memory). After printing the variable as a string, we can guess that the format is: YYYYMMDDhhmm. The **lubridate** [@R-lubridate] package is designed to facilitate processing date and time objects. Knowing the format of the timestamp variables in our dataset, we can use `ymd_hm()` to convert them to actual date-time objects.

```{r, message=FALSE}
dates <- ymd_hm(hhdf$TIMESTAMP_START)
dates[1]
```

Working with such date-time objects facilitates typical operations on time series. For example, adding one day can be done by:

```{r}
nextday <- dates + days(1)
nextday[1]
```

The following returns the month of each date object:

```{r}
month(dates[1])
```

The number 1 stands for the month of the year, i.e., January. You can find more information on formatting dates and time within the **tidyverse** [here](https://r4ds.had.co.nz/dates-and-times.html), and a complete reference of the **lubridate** package is available [here](https://lubridate.tidyverse.org/).

### Variable (re-) definition

Since `read_csv()` did not interpret the `TIMESTAMP_*` variables as desired, we may convert the entire column in the data frame into a date-time object. In base-R, we would do this by:

```{r eval=FALSE}
hhdf$TIMESTAMP_START <- ymd_hm(hhdf$TIMESTAMP_START)
```

Modifying existing or creating new variables (columns) in a data frame is done in the tidyverse using the function `mutate()`. The equivalent statement is:

```{r eval=FALSE}
hhdf <- mutate(hhdf, TIMESTAMP_START = ymd_hm(TIMESTAMP_START))
```

Note that in the code chunk above, the function `mutate()` is from the tidyverse package *dplyr*. It takes a dataframe as its first argument (here `hhdf`) and returns a dataframe as its output. You will encounter an alternative, but equivalent, syntax in the following form:

```{r, eval=FALSE}
hhdf <- hhdf |> 
  mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START))
```

Here, the pipe operator `|>` is used. It "pipes" the object evaluated on its left side into the function on its right side, where the object takes the place of (but is not spelled out as) the first argument of that function. Using the pipe operator can have the advantage of facilitating the separation, removal, inserting, or re-arranging of individual transformation steps. Arguably, it facilitates reading code, especially for complex data transformation workflows. Therefore, you will encounter the pipe operator frequently throughout the remainder of this course.

Mutating both our timestamp variables could be written as `mutate(TIMESTAMP_START = ymd_hm(TIMESTAMP_START), TIMESTAMP_END = ymd_hm(TIMESTAMP_END))`. Sometimes, such multiple-variable mutate statements can get quite long. A handy short version of this can be implemented using `across()`:

```{r}
hhdf <- hhdf |> 
  mutate(across(starts_with("TIMESTAMP_"), ymd_hm))
```

We will encounter more ways to use mutate later in this tutorial. A complete reference to `mutate()` is available [here](https://r4ds.had.co.nz/transform.html#add-new-variables-with-mutate).

### Axes of variation

Tabular data is two-dimensional (rows $\times$ columns), but not all two-dimensional data is tabular. For example, raster data is a two-dimensional array of data (a matrix) representing variables on an evenly spaced grid, for example pixels in remotely sensed imagery. For example the `volcano` data (provided as an example dataset in R) is a 2-dimensional array, each column contains the same variable, and no variable names are provided.

```{r}
volcano[1:5, 1:5]
```

In the `volcano` dataset, rows and columns represent different geographic positions in latitude and longitude, respectively. The `volcano` data is not tabular data. Another typical example for non-tabular data are climate model outputs. They are typically given as *arrays* with more than two dimensions. Typically, this is longitude, latitude, and time, and sometimes a vertical dimension representing, for example, elevation. Such data is multi-dimensional and, as such, not tabular.

Tabular data, although formatted in two dimensions by rows and columns, may represent data that varies along multiple axes. Most environmental data is *structured*, that is, values of "nearby" observations tend to be more similar than values of "distant" observations. Here, "nearby" and "distant" may refer to a spatial distance, but not necessarily so. Structure in data arises from similarity of the subjects generating the data (e.g., evapotranspiration over two croplands may be more similar than evapotranspiration over a forest), or from temporal proximity. In biological data, there may be a genetic structure arising from evolutionary relatedness ([Roberts et al., 2016](https://onlinelibrary.wiley.com/doi/10.1111/ecog.02881)). Note also that temporal proximity is more complex than than being governed by a single dimension - time. In environmental data, time is often expressed through periodically varying conditions (the diurnal and seasonal cycles). It's often critical to understand and account for the structure in data when analysing it and using it for model fitting. Challenges are posed when structure is not apparent or not known.

Note also that some structures are *hierarchical*. For example, data may be structured by postal codes within cantons; or by hours within a day within a year). Biological data may be generated by species within *genera* within *families*. Data from experiments is typically structured as *samples* within *treatments*. Etc. You see, structure in data is rather the rule than the expception.

Our example data contains values recorded at each half-hourly time interval over the course of eleven years (check by `nrow(hhdf)/(2*24*365)`). The data is recorded at a site, located in the temperate climate zone, where solar radiation and therefore also other meteorological variables and ecosystem fluxes vary substantially over the course of a day and over the course of a year. Although not explicitly separated, the date-time object thus encodes information along multiple *axes of variation* in the data. For example, over the course of one day (`2*24` rows in our data), the shortwave incoming radiation `SW_IN_F` varies over a typical diurnal cycle:

```{r}
plot(hhdf[1:(2*24),]$TIMESTAMP_START, hhdf[1:(2*24),]$SW_IN_F, type = "l")
```

Over the course of an entire year, shortwave incoming radiation varies with the seasons, peaking in summer:

```{r}
plot(hhdf[1:(365*2*24),]$TIMESTAMP_START, hhdf[1:(365*2*24),]$SW_IN_F, type = "l")
```

All data frames have two dimensions, rows and columns. Our data frame is organised along half-hourly time steps in rows. As described above, these time steps belong to different days, months, and years, although these "axes of variation" are not reflected by the structure of the data frame and we do not have columns that indicate the day, month or year of each half-hourly time step. This would be redundant information since the date-time objects of columns `TIMESTAMP_*` contain this information. However, for certain applications, it may be useful to separate information regarding these axes of variation more explicitly. For example by:

```{r}
hhdf |>
  mutate(year = year(TIMESTAMP_START),
         month = month(TIMESTAMP_START)) |>
  select(TIMESTAMP_START, TIMESTAMP_END, year, month)  # for displaying
```

Note that we used `mutate()` here to create a new variable (column) in the data frame, as opposed to above where we overwrote an existing variable with the same function.

### Tidy data

Data comes in many forms and shapes. For example, Excel provides a playground for even the wildest layouts of information in some remotely tabular form and merged cells as we will see in the Exercises. A data frame imposes a relatively strict formatting in named columns of equal length. But even data frames can come in various shapes - even if the information they contain is the same.

```{r echo=FALSE}
## this is hidden - used to create the objects printed in next chunk
## using the ts object 'co2'
df1 <- tibble(co2_concentration = as.vector(co2), 
       year_dec = time(co2)
       ) |> 
  mutate(month_dec = year_dec - as.integer(year_dec)) |> 
  mutate(year = as.integer(year_dec - month_dec)) |> 
  mutate(month = as.integer(month_dec * 12)) |> 
  mutate(date = ymd(paste0(as.character(year), as.character(month + 1), "-15"))) |> 
  mutate(month = month(date, label = TRUE)) |> 
  select(year, month, co2_concentration) |> 
  slice(1:36)

df2 <- df1 |> 
  pivot_wider(names_from = month, values_from = co2_concentration)

df3 <- df1 |> 
  # mutate(year = as.character(year)) |> 
  pivot_wider(names_from = year, values_from = co2_concentration)
```

```{r}
df1
df2
df3
```

There are advantages for interoperability and ease of use when data frames come with consistent layouts, adhering to certain design principles. We have learned that in tabular data, each row contains the same number of columns and each column contains the same type of values (for example numeric, or characters). And that each row can be regarded as a separate instance of the same type, for example a record of simultaneously taken measurements, along with some attributes. Following these principles strictly leads to *tidy data*. In essence, quoting [Wickham](https://r4ds.had.co.nz/tidy-data.html), data is tidy if:

-   Each variable has its own column.
-   Each observation has its own row.
-   Each value has its own cell.

![Rules for tidy data](./figures/tidy_data.png) <!-- Figure from https://r4ds.had.co.nz/tidy-data.html --> <!-- Figure URL: https://d33wubrfki0l68.cloudfront.net/6f1ddb544fc5c69a2478e444ab8112fb0eea23f8/91adc/images/tidy-1.png -->

The concept of tidy data can even be taken further by understanding a "value" as any object type, e.g. a list or a data frame. This leads to a list or data frame "nested" within a data frame. You will learn more about this below.

The concept of tidy data can even be taken further by understanding a "value" as any object type, e.g. a list or a data frame. This leads to a list or data frame "nested" within a data frame. You will learn more about this below.

### Aggregating data

Aggregating data refers to collapsing a larger set of values into a smaller set of values that are derived from the larger set. For example, we can aggregate over all $N$ rows in a data frame ($N\times M$), calculating the sum for each of the $M$ columns. This returns a data frame ($1 \times M$) with the same number of columns as the initial data frame, but only one row. Often, aggregations are done not across all rows but for rows within $G$ groups of rows. This yields a data frame ($G \times M$) with the number of rows corresponding to the number of groups.

Let's say we want to calculate the mean of half-hourly shortwave radiation within each day. That is, to aggregate our half-hourly data to daily data by taking a mean. There are two pieces of information needed for an aggregation step: The factor (or "axis of variation"), here days, that groups a vector of values for collapsing it into a single value, and the function used for collapsing values, here, the `mean()` function. This function should take a vector as an argument and return a single value as an output. These two steps are implemented by the *dplyr* functions `group_by()` and `summarise()`. The entire aggregation workflow is implemented by the following code:

```{r}
ddf <- hhdf |>  
  mutate(date = as_date(TIMESTAMP_START)) |>  # converts the ymd_hm-formatted date-time object to a date-only object (ymd)
  group_by(date) |> 
  summarise(SW_IN_F = mean(SW_IN_F))
```

The seasonal course can now be more clearly be visualized with the data aggregated to daily values.

```{r}
plot(ddf[1:365,]$date, ddf[1:365,]$SW_IN_F, type = "l")
```

We can also apply multiple aggregation functions to different variables simultaneously. In the example below, we aggregate half-hourly data to daily data by...

-   taking the daily mean GPP

-   counting the number of half-hourly data points by day

-   counting the number of measured (not gap-filled) data points

-   taking the mean shortwave radiation

Finally, we calculate the fraction of measured underlying half-hourly data from which the aggregation is calculated and we save the daily data frame as a CSV file for later use.

```{r}
ddf <- hhdf |> 
    mutate(date = as_date(TIMESTAMP_START)) |>   # converts time object to a date object
    group_by(date) |> 
    summarise(GPP_NT_VUT_REF = mean(GPP_NT_VUT_REF, na.rm = TRUE),
              n_datapoints = n(), # counts the number of observations per day
              n_measured = sum(NEE_VUT_REF_QC == 0), # counts the number of actually measured data (excluding gap-filled and poor quality data)
              SW_IN_F = mean(SW_IN_F, na.rm = TRUE),  # we will use this later
              .groups = 'drop' # to un-group the resulting data frame
              ) |> 
    mutate(f_measured = n_measured / n_datapoints) # calculate the fraction of measured values over total observations
write_csv(ddf, file = "data/ddf.csv")
ddf
```

More info on how to group values using summarise functions [here](https://r4ds.had.co.nz/transform.html#grouped-summaries-with-summarise), or a summary on the inputs the function [group_by()](https://dplyr.tidyverse.org/reference/group_by.html) and [summarise()](https://dplyr.tidyverse.org/reference/summarise.html) take.

Let's

Aggregating is related to *nesting* performed by the *tidyr* function `nest()`:

```{r message=FALSE}
hhdf |>  
  mutate(date = as_date(TIMESTAMP_START)) |>
  group_by(date) |> 
  nest()
```

Here, the data frame has one row per date and therefore the same number of rows as the data frame `ddf`, but the data itself is not reduced by a summarising function. Instead, the data is kept at the half-hourly level, but it's nested inside the new column `data`, which now contains a list of half-hourly data frames for each day. This is just a brief perspective of what nesting is about. More is explained in the Section Extra material below. More comprehensive tutorials on nesting and functional programming are available in [Altman, Behrman and Wickham (2021)](https://dcl-prog.stanford.edu/) or in [Wickham & Grolemund (2017), Chapter 21](https://r4ds.had.co.nz/iteration.html).

### Data cleaning

Data cleaning is often a time-consuming task and decisions taken during data cleaning may be critical for analyses and modelling. In the following, we distinguish between cleaning formats, the identification (and removal) of "bad" data, and the gap-filling of missing or removed data. An excellent source for further reading is the [Quartz Guide to Bad Data](https://github.com/Quartz/bad-data-guide) which provides an overview of how to deal with different types of bad data.

#### Cleaning formats

As a general principle, we want to have *machine readable* data. Key for achieving machine-readability is that a cell should only contain one value of one type. Hence, for example, character strings should be kept in separate columns (as separate variables) from numeric data. Character strings can impose particular challenges for achieving machine-readability. Typically, they encode categorical or ordinal information, but are prone to spelling inconsistencies or errors that undermine the ordering or categorization. Here are typical examples for challenges working with character strings and lessons for avoiding problems:

-   Often, character strings encode the units of a measurement, and entries are `c("kg m-2", "kg/m2", "Kg / m2", "1000 g m-2")` . They are all equivalent, but "the machine" treats them as non-identical. To clean such data, one may compile a lookup-table to identify equivalent (but not identical) strings. Much better is to specify a consistent treatment of units before data collection.

-   Even if the data is clean and contains a consistently spelled categorical variable in the form of a character string, R doesn't necessarily treat it as categorical. For certain downstream steps of the workflow, it may be necessary to transform such a variable to one of type `factor`. For example, as entries of an unordered categorical variable, we have `unique(df$gender) = c("female", "male", "non-binary")`. To treat them as categorical and not just mere character strings, we would have to do:

    ```{r, eval=FALSE}
    df <- df |> mutate(gender = as.factor(gender))
    ```

-   Character strings may encode ordinal information. For example, entries specify quality control information and are one of `c("good quality", "fair quality, "poor quality")`. A challenge could be that the spelling is inconsistent (`c("Good quality", "good quality", …)`). Using integers (positive natural numbers) instead of character strings avoids such challenges and enforces an order. The quality control variable `NEE_VUT_REF_QC` in our example dataset `hhdf` follows this approach:

    ```{r}
    unique(hhdf$NEE_VUT_REF_QC)
    ```

-   An entry like `>10 m` is not a friend of a data scientist. Here, we have three pieces of information: `>` as in "greater than", `10`, and `m` indicating the units. A machine-readable format would be obtained by creating separate columns for each piece of information. The `>` should be avoided already at the stage of recording the data. Here, we may have to find a solution for encoding it in a machine readable manner (see Exercises).

-   Can you think of more such examples? (-\> Exercises)

String manipulations are usually required for cleaning data. The section Strings below demonstrates some simple examples.

Note that a majority of machine learning algorithms and other statistical model types require all data to be numeric. Methods exist to convert categorical data into numeric data, as we will learn later. We re-visit data cleaning in the form of data *preprocessing* as part of the modelling workflow in Chapter \@ref(supervised_ml).

#### Bad data

Data may be "bad" for different reasons, including sensor error, human error, a data point representing a different population, or unsuitable measurement conditions. In this sense, data is "bad" if it doesn't represent what it is assumed to represent. Its presence in analyses and modelling may undermine the model skill or even lead to spurious results. A goal of data cleaning typically is to remove bad data. But how to detect them? And how safe is it to remove them?

A diversity of processes may generate bad data and it is often not possible to formulate rules and criteria for their identification *a priori*. Therefore, an understanding of the data and the data generation processes is important for the identification and treatment of bad data. Often, such an understanding is gained by repeated exploratory data analysis cycles, involving the visualization, transformation, and analysis of the data.

Ideally, information about the quality of the data is provided as part of the dataset. Also other meta-information (e.g., sensor type, human recording the data, environmental conditions during the data collection) may be valuable for data cleaning purposes. In our example dataset, the column with suffices `_QC` provide such information (see 'Example data' section above) and an example for their use in data cleaning is given further below.

Bad data may come in the form of *outliers*, which are commonly defined based on their value with respect to the *distribution* of all values of the same variable in a dataset. Hence, their identification most commonly relies on quantifying their distance from the center of the variable's empirical distribution. The default `boxplot()` plotting function in R (which we will learn about more in Chapter \@ref(data_vis)) shows the median (bold line in the center), the upper and lower quartiles (corresponding to the 25% and the 75% quantiles, often referred to as $Q_1$ and $Q_3$ , given by the upper and lower edge of the box plot) and the range of $( Q_1 - 1.5 (Q_3 - Q_1), Q_3 + 1.5 (Q_3 - Q_1))$. Any point outside this range is plotted by a circle and labeled an "outlier". However, this definition is very restrictive and may lead to a false labeling of outliers, in particular if they are drawn from a distribution with a fat tail or from asymmetrical distributions.

Outliers may also be identified via multivariate distributions. We will re-visit such methods later, in Chapter \@ref(regressionclassification). For certain applications, outliers or *anomalies* may be the target of the investigation, not the noise in the data. This has spurred the field of [*anomaly detection*](https://en.wikipedia.org/wiki/Anomaly_detection) which relies on machine learning algorithms for determining whether a value is anomalous, given a set of covariates.

Sensor error may generate spurious values, identified, for example when a continuous variable attains the numerically identical value with a spuriously high frequency.

XXX spurious?

```{r}
hhdf$GPP_NT_VUT_REF |> 
  table() |> 
  sort(decreasing = TRUE) |> 
  head()
```

Other processes may lead to spurious trends or *drift* in the data, for example caused by sensor degradation. Spurious *step changes* or *change points* in time series or in (multivariate) regressions may be related to the replacement or deplacement of the measuring device. Different methods and R libraries help identifying such cases (see for example [this](https://www.marinedatascience.co/blog/2019/09/28/comparison-of-change-point-detection-methods/) tutorial). Solutions have to be found for the remediation of such spurious patterns in the data on a case-by-case basis.

#### Handling missing data

The question about when data is "bad" and whether to remove it is often critical. Such decisions are important to keep track of and should be reported as transparently as possible in publications. In reality, where the data generation process may start in the field with actual human beings writing notes in a lab book, and where the human collecting the data is often not the same as the human analyzing the data or writing the paper, it's often more difficult to keep track of such decisions. As a general principle, it is advisable to design data records such that decisions made during the data collection process remain transparent throughout all stages of the workflow and that sufficient information be collected to enable later revisions of particularly critical decisions. In practice, this means that the removal of data and entire rows should be avoided and implemented only at the very last step if necessary (e.g., when passing the data into a model fitting function). Instead, information about whether data is bad or not should be kept in a separate, categorical, variable (a *quality control* variable, like `*_QC` variables in our example data `hhdf`).

Data may be missing for several reasons. Some yield random patterns of missing data, others not. In the latter case, we can speak of *informative missingness* (Kuhn & Johnson, 2003) and its information can be used for modelling. For categorical data, we may replace such data with `"none"` (instead of `NA`), while randomly missing data may be dropped altogether. Some machine learning algorithms (mainly tree-based methods, e.g., Random Forest) can handle missing values. However, when comparing the performance of alternative ML algorithms, they should be tested with the same data and removing missing data should be done beforehand.

Most machine learning algorithms require missing values to be removed. That is, if any of the cells in one row has a missing value, the entire cell gets removed. This generally leads to a loss of information contained in the remaining variables that are not missing. Methods exist to *impute* missing values in order to avoid this information loss. However, the gain of data imputation has to be traded off against effects of associating the available variables with the imputed (knowingly wrong) values, and effects of *data leakage* have to be considered. Data imputation as part of the modelling process will be dealt with in Chapter \@ref(supervised_ml).

In our example dataset, some values of `SWC_F_MDS_*` are given as `-9999`.

```{r}
hhdf |> 
  select(TIMESTAMP_START, starts_with("SWC_F_MDS_")) |> 
  head()
```

When reading the documentation of this specific dataset, we learn that `-9999` is the code for *missing data*. The *dplyr* functions help us to clarify these missing values by mutating across all numeric variables and overwrite entries with `NA` if they hold a `-9999`.

```{r}
hhdf <- hhdf |>  
  mutate(across(where(is.numeric), ~na_if(., -9999)))

hhdf |> 
  select(TIMESTAMP_START, starts_with("SWC_F_MDS_")) |> 
  head()
```

This lets us visualise the data and its gaps with `vis_miss()` from the *visdat* package. Visualising missing data can be informative for making decisions about dropping rows with missing data versus removing predictors from the analysis (which would imply too much data removal).

```{r warning=FALSE, message=FALSE}
visdat::vis_miss(
  hhdf |> slice(1:10000),
  cluster = FALSE, 
  warn_large_data = FALSE
  )
```

For many applications, we want to filter the data so that the values of particular variables satisfy certain conditions. The *dplyr* function used for such tasks is `filter()`. As argument, it takes the expressions that specify the criterion for filtering using logical operators (`>, >=, <, ==, !-, ...`, see Chapter \@ref(getting_started)). Multiple filtering criteria can be combined with logical (boolean) operators:

-   `&`: logical AND
-   `|`: logical OR
-   `!` logical NOT

For example, if we wanted only those rows in our data where NEE is based on measured or good quality gap-filled NEE data, we write:

```{r eval=FALSE}
hhdf |>
  filter(NEE_VUT_REF_QC == 0 | NEE_VUT_REF_QC == 1)
```

For evaluating multiple OR operations simultaneously, we can write alternatively and equivalently:

```{r eval=FALSE}
hhdf |>
  filter(NEE_VUT_REF_QC %in% c(0,1))
```

Note that `filter()` removes entire rows. In some cases this is undesired and it is preferred to replace bad quality values with `NA`. It is important to note that specifying a value as missing is information itself. Dropping an entire row leads to the loss of this information. For cases where we do not want to drop entire rows when applying `filter()`, we can just replace certain values with `NA`. In our case, where we want to retain only data where NEE is based on actual measurements or good quality gap-filling, we can do this by:

```{r eval=FALSE}
hhdf |> 
    mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC %in% c(0,1), GPP_NT_VUT_REF, NA))
```

If we decide to drop a row containing `NA` in any of the variables later during the workflow, we can do this, for example using the useful *tidyr* function `drop_na()`.

```{r eval=FALSE}
hhdf |> 
  drop_na()
```

An excellent source for a more comprehensive introduction to missing data handling is given in [Kuhn & Johnson](https://bookdown.org/max/FES/handling-missing-data.html).

After having applied some data reduction and cleaning steps above, let's save the data frame in the form of a CSV file for use in later chapters.

```{r}
write_csv(hhdf, file = "data/FLX_CH-Lae_FLUXNET2015_FULLSET_HH_2004-2006_CLEAN.csv")
```

### Combining relational data

Often, data is spread across multiple files and tables and needs to be combined for the planned analysis. In the simplest case, data frames have a corresponding set of columns and we can "stack" them along rows:

```{r echo=FALSE}
df4 <- df3[1:6,]
df5 <- df3[7:12,]
```

```{r}
df4
df5
bind_rows(df4, df5)
```

..., or data frames have a corresponding set of rows (and in the same order) and we can "stack" them along columns

```{r echo=FALSE}
df6 <- df3[, c(1,2,3)]
df7 <- df3[, c(1,4)]
```

```{r}
df6
df7
bind_cols(df6, df7)
```

But beware! In particular the stacking along columns (`bind_cols()`) is very error-prone. Since a tidy data frame regards each row as an instance of associated measurements, the rows of the two data frames and their order must match exactly. Otherwise, an error is raised or (even worse) rows get associated when they shouldn't be. In such cases, where information about a common set of observations is distributed across multiple data objects, we are dealing with *relational data*. The key for their combination (or "merging") is a *unique identification key* - the column that is present in both data frames and which contains values along which the merging of the two data frames is performed. In our example from above, this is `month`, and we can use the dplyr function `left_join()`.

```{r}
df6 |>
  slice(sample(1:n(), replace = FALSE)) |>  # re-shuffling rows
  left_join(df7, by = "month") |> 
  arrange(month)  # sort in ascending order
```

Note that here, we first re-shuffled (permuted) the rows of `df6` for demonstration purposes, and arranged the output data frame again by `month` - an ordinal variable. `left_join()` is not compromised by the order of the rows, but instead relies on the unique identification key, specified by the argument `by = "month"`, for associating (merging, joining) the two data frames. In some cases, multiple columns may act as the unique identification key in their combination (for example `by = c("year", "month")`).

Other variants of `*_join()` are available as described [here](https://r4ds.had.co.nz/relational-data.html).

## Extra material

### Functional programming I

Above, we read a CSV table into R and applied several data transformation steps. In practice, we often have to apply the same data transformation steps repeatedly over a set of similar objects. This *extra material* section outlines an example workflow for demonstrating how to efficiently work with lists of similar objects - in particular, lists of data frames.

Our aim is to read a set of files into R data frames and apply transformation steps to each data frame separately. Here, we will work with daily data, not half-hourly data. The daily data contains largely identical variables with consistent naming and units as in the half-hourly data (description above). Let's start by creating a list of paths that point to the files with daily data. They are all located in the directory `"./data"` and share a certain string of characters in their file names `"_FLUXNET2015_FULLSET_DD_"`.

```{r}
vec_files <- list.files("./data", pattern = "_FLUXNET2015_FULLSET_DD_", full.names = TRUE)
print(vec_files)
```

`vec_files` is now a vector of three files paths as character strings. To read in the three files and combine the three data frames (`list_df` below) into a list of data frames, we could use a `for` loop:

```{r message=FALSE, eval=FALSE}
list_df <- list()
for (ifil in vec_files){
  list_df[[ifil]] <- read_csv(ifil)
}
```

Repeatedly applying a function (here `read_csv()`) over a list similar objects is facilitated by the `map*()` family of functions from the *purrr* package. An (almost) equivalent statement is:

```{r message=FALSE}
list_df <- map(as.list(vec_files), ~read_csv(.))
```

Here, `map()` applies the function `read_csv()` to elements of a *list*. Hence, we first have to convert the vector `vec_files` to a list. A list is always the first argument within the `map()` function. Note two new symbols (`~` and `.`). The `~` always goes before the function that is repeatedly applied (or "mapped") to elements of the list. The `.` indicates where the elements of the list would go if spelled out (e.g., here, `read_csv(.)` would be `read_csv("./data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv")` for the first iteration). The output of `map()` is again a list. There are many variants of the function `map()` that each have a specific use. A complete reference for all *purrr* functions is available [here](https://purrr.tidyverse.org/reference/index.html). A useful and more extensive tutorial on purrr is available [here](https://www.r-bloggers.com/one-stop-tutorial-on-purrr-package-in-r/).

The above `map()` call does not return a *named* list as our `for` loop created. But we can give each element of the returned list of data frames different names by:

```{r}
names(list_df) <- vec_files  # this makes it a named list
```

Next, we will apply a similar data cleaning procedure to this data set as we did above for half-hourly data. To do so, we "package" the individual cleaning steps into a function ...

```{r}
# function definition
clean_data_dd <- function(df){

  df <- df |>

    # select only the variables we are interested in
    select(
      TIMESTAMP,
      ends_with("_F"),
      GPP_NT_VUT_REF,
      NEE_VUT_REF_QC,
      starts_with("SWC_F_MDS_"),
      -contains("JSB")) |> 

    # convert to a nice date object
    mutate(TIMESTAMP = lubridate::ymd(TIMESTAMP)) |>

    # set all -9999 to NA
    mutate(across(where(is.numeric), ~na_if(., -9999)))
  
  return(df)
}
```

... and apply this function to each data frame within our list of data frames:

```{r}
list_df <- map(list_df, ~clean_data_dd(.))
```

Having different data frames as elements of a list may be impractical. Since we read in similarly formatted files and selected always the same variables in each data frame, all elements of the list of data frames `list_df` share the same columns. This suggests that we can collapse our list of data frames and "stack" data frames along rows. As described above, this can be done using `bind_rows()` and we can automatically create a new column `"siteid"` in the stacked data frame that takes the name of the corresponding list element.

```{r}
ddf_allsites <- bind_rows(list_df, .id = "siteid")
ddf_allsites
```

A visualisation of missing data indicates that soil water content data (`SWC_F_MDS_*`) are often missing.

```{r}
visdat::vis_miss(
  ddf_allsites |> slice(1:10000),
  cluster = FALSE,
  warn_large_data = FALSE
  )
```

### Strings

The column `siteid` currently contains strings specifying the full paths of the files that were read in earlier. The next task is to extract the site name from these strings. The file names follow a clear pattern (this also highlights why naming files wisely can often make life a lot simpler).

```{r}
ddf_allsites$siteid |> head()
```

The paths each start with the subdirectory where they are located (`"./data/"`), then `"FLX_"`, followed by the site name (the first three entries of the table containing data from all sites are for the site `"CH-Dav"`), and then some more specifications, including the years that respective files' data cover.

The [*stringr*](https://stringr.tidyverse.org/) package [@R-stringr] (part of tidyverse) offers a set of functions for working with strings. [Wikham (XXX)](https://r4ds.had.co.nz/strings.html) provide a more comprehensive introduction to working with strings. Here, we would like to extract the six characters, starting at position 12. The function `str_sub()` does that job.

```{r}
vec_sites <- str_sub(vec_files, start = 12, end = 17)
head(vec_sites)
```

We can use this function to mutate all values of column `"siteid"`, overwriting it with just these six characters.

```{r}
ddf_allsites <- ddf_allsites |>
  mutate(siteid = str_sub(siteid, start = 12, end = 17))

ddf_allsites
```

### Functional programming II

Functions can be applied to a list of objects of any type. Therefore, `map()` is a powerful approach to "iterating" over multiple instances of the same object type and can be used for all sorts of tasks. In the following, list elements are data frames of daily data and the function `lm()` fits a linear regression model of GPP versus shortwave radiation to each sites' data. We'll learn more about fitting statistical models in R in Chapter \@ref(regressionclassification).

```{r}
list_linmod <- map(list_df, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .))
```

Note how the `.` indicates where the elements of `list_df` go when evaluating the `lm()` function. This returns a list of linear model objects (the type of objects returned by the `lm()` function call).

We can spin the functional programming concept further and apply (or map) the `summary()` function to the `lm`-model objects to get a list of useful statistics and metrics, and then further extract the element `"r.squared"` from that list as:

```{r}
list_linmod |>
  map(summary) |>       # applyting a function
  map_dbl("r.squared")  # extracting from a named list
```

`map_dbl()` is a variant of the `map()` function that returns not a list, but a vector of numeric values of class "double" (hence, the name `_dbl`). Note further, that providing a character (`"r.squared"`) as an argument instead of an (unquoted) function name, `map()` extracts the correspondingly named list element, instead of applying a function to a list element.

When writing code for an analysis, it's useful, if not essential, to understand the objects we're working with, understand its type and shape, and make sense of the results of simple `print <object>` statements. Data frames are particularly handy as they provide an organisation of data that is particularly intuitive (variables along columns, observations along rows, values in cells). Here, we're dealing with a list of linear model objects. Can such a list fit into the paradigm of *tidy* data frames?

Yes, they can. Think of the linear model objects as 'values'. Values don't necessarily have to be scalars, but they can be of any type (class).

```{r}
tibble(
  siteid = vec_sites,
  linmod = list_linmod
  )
```

The fact that cells can contain any type of object offers a powerful concept. Instead of a linear model object as in the example above, each cell may even contain another data frame. In such a case, we say that the data frame is no longer *flat*, but *nested*.

The following creates a nested data frame, where the column `data` is defined by the list of data frames read from files above (`list_df`).

```{r}
tibble(
  siteid = vec_sites,
  data = list_df
  )
```

We can achieve the same result by directly nesting the flat data frame holding all sites' data (`ddf_allsites`). This is done by combining the `group_by()`, which we have encountered above when aggregating using `summarise()`, with the function `nest()` from the *tidyr* package.

```{r}
ddf_allsites |>
  group_by(siteid) |>
  nest()
```

The function `nest()` names the nested data column automatically `"data"`.

This structure is very useful. For example, for applying functions over sites' data frames separately (and not over the entire data frame). By combining `map()` and `mutate()`, we can fit linear models on each site's data frame individually in one go.

```{r eval=FALSE}
ddf_allsites |>
  group_by(siteid) |>
  nest() |>
  mutate(linmod = map(data, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .)))
```

This approach is extremely powerful and lets you stick to working with tidy data frames and use the rows-dimension flexibly. Here, rows are sites and no longer time steps, while the nested data frames in column `"data"` have time steps along their rows. The power of nesting is also to facilitate complex aggregation steps over a specified dimension (or axis of variation, here given by `siteid`), where the aggregating function is not limited to taking a vector as input and returning a scalar, as is the case for applications of `summarise()` (see above).

Combining the steps described above into a single workflow, we have:

```{r}
ddf_allsites_nested <- ddf_allsites |>
  group_by(siteid) |>
  nest() |>
  mutate(linmod = map(data, ~lm(GPP_NT_VUT_REF ~ SW_IN_F, data = .))) |>
  mutate(summ = map(linmod, ~summary(.))) |>
  mutate(rsq = map_dbl(summ, "r.squared")) |>
  arrange(desc(rsq))  # to arrange output, with highest r-squared on top

ddf_allsites_nested
```

This code is a demonstration of the power of tidy and nested data frames and for the clarity of the *tidyverse* syntax.

Nesting is useful also for avoiding value duplication when joining relational data objects. Above, we nested time series data objects (where time steps and sites are both organised along rows) by sites and got a data frame where only sites are organised along rows, while time steps are nested inside the column `"data"`. This now fits the structure of a relational data object (`siteinfo_fluxnet2015`) containing site-specific meta information (also with only sites along rows).

```{r}
base::load("data/siteinfo_fluxnet2015.rda")  # loads siteinfo_fluxnet2015
```

Joining the nested data frame with site meta information results in a substantially smaller and much handier data frame compared to an alternative, where the site meta information is joined into the un-nested (daily) data frame, and therefore duplicated for each day within sites.

```{r}
ddf_allsites_nested_joined <- siteinfo_fluxnet2015 |>
  rename(siteid = sitename) |>
  right_join(select(ddf_allsites_nested, -linmod, -summ, -rsq),
             by = "siteid")

ddf_allsites_joined <- siteinfo_fluxnet2015 |>
  rename(siteid = sitename) |>
  right_join(ddf_allsites,
             by = "siteid")

print(paste("Flat and joined:", 
            format(object.size(ddf_allsites_joined),  
                   units = "auto", 
                   standard = "SI")))
print(paste("Nested and joined:", 
            format(object.size(ddf_allsites_nested_joined),  
                   units = "auto", 
                   standard = "SI")))

# save for later use
write_rds(ddf_allsites_nested_joined, file = "data/ddf_allsites_nested_joined.rds")
```

## Exercises

-   find a way to encode `>` in `>10 m` in machine-readable way.
-   Exercise: hhdf cleaning based on QC info, non-random missingness (more at night), removing data implies shift of the mean.
-   aggregate the `CO2` data, calculating the mean by `Treatment.`

```{r}
CO2 |> 
  group_by(Treatment) |> 
  summarise(uptake = mean(uptake))
```

-   make the following tables tidy:

    -   `df_sunspots`

    ```{r}
    df_sunspots <- tibble(count = as.vector(sunspots), 
           year_dec = time(sunspots)
           ) |> 
      mutate(month_dec = year_dec - as.integer(year_dec)) |> 
      mutate(year = year_dec - month_dec) |> 
      mutate(month = as.integer(month_dec * 12)) |> 
      mutate(date = ymd(paste0(as.character(year), as.character(month + 1), "-15"))) |> 
      mutate(month = month(date, label = TRUE)) |> 
      select(year, month, count)
    ```

    -   [link](https://stineb.github.io/esds_book/ch-02.html#exercise-1)

-   Exercises: identify outliers in VPD and temperature in our example dataset.

```{r}
hhdf |> 
  select(TA_F, VPD_F) |> 
  boxplot()

# the function used to calculate boxplot statistics
# returning "outliers" in TA_F
hhdf$TA_F[hhdf$TA_F %in% boxplot.stats(hhdf$TA_F)$out] |> head()

# returning "outliers" in VPD_F
hhdf$VPD_F[hhdf$VPD_F %in% boxplot.stats(hhdf$VPD_F)$out] |> head()
```

-   combine `df_sunspots` and `df_co2` along years and months.

```{r}
df_co2 <- tibble(co2_concentration = as.vector(co2), 
       year_dec = time(co2)
       ) |> 
  mutate(month_dec = year_dec - as.integer(year_dec)) |> 
  mutate(year = year_dec - month_dec) |> 
  mutate(month = as.integer(month_dec * 12)) |> 
  mutate(date = ymd(paste0(as.character(year), as.character(month + 1), "-15"))) |> 
  mutate(month = month(date, label = TRUE)) |> 
  select(year, month, co2_concentration)
```

## Solutions
